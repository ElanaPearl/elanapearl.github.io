<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> the bug that taught me more about PyTorch than years of using it | Elana Simon </title> <meta name="author" content="Elana P. Simon"> <meta name="description" content="a loss plateau that looked like my mistake turned out to be a PyTorch bug. tracking it down meant peeling back every layer of abstraction, from optimizer internals to GPU kernels."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-one-light.css?d02c04e036cd4bccb1d6caf420c1238f" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-one-dark-pro.css?5348b8a3ac453c5c67b40d4dedf74566" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css" integrity="sha256-3qTIuuUWIFnnU3LpQMjqiXc0p09rvd0dmj+WkpQXSR8=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> <script>!function(e,t,n,s,c,a,i){e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},(a=t.createElement(n)).async=1,a.src=s,(i=t.getElementsByTagName(n)[0]).parentNode.insertBefore(a,i)}(window,document,"script","https://assets.mailerlite.com/js/universal.js","ml"),ml("account","1858211");</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <meta property="og:type" content="website"> <meta property="og:url" content="https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/"> <meta property="og:title" content="the bug that taught me more about PyTorch than years of using it"> <meta property="og:description" content="a loss plateau that looked like my mistake turned out to be a PyTorch bug. tracking it down meant peeling back every layer of abstraction, from optimizer internals to GPU kernels."> <meta property="og:image" content="https://elanapearl.github.io/assets/img/the_bug_that_taught_me_pytorch_post/iceberg_meme.png"> <meta property="og:image:width" content="2126"> <meta property="og:image:height" content="1478"> <meta property="twitter:card" content="summary_large_image"> <meta property="twitter:url" content="https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/"> <meta property="twitter:title" content="the bug that taught me more about PyTorch than years of using it"> <meta property="twitter:description" content="a loss plateau that looked like my mistake turned out to be a PyTorch bug. tracking it down meant peeling back every layer of abstraction, from optimizer internals to GPU kernels."> <meta property="twitter:image" content="https://elanapearl.github.io/assets/img/the_bug_that_taught_me_pytorch_post/iceberg_meme.png"> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "the bug that taught me more about PyTorch than years of using it",
            "description": "a loss plateau that looked like my mistake turned out to be a PyTorch bug. tracking it down meant peeling back every layer of abstraction, from optimizer internals to GPU kernels.",
            "published": "October 22, 2025",
            "authors": [
              
              {
                "author": "Elana Simon",
                "authorURL": "https://www.elanapearl.github.io",
                "affiliations": [
                  {
                    "name": "Stanford University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%65%6C%61%6E%61%73%69%6D%6F%6E%39%35@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=qc6CJjYAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/ElanaPearl" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://twitter.com/ElanaPearl" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/"> blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill-wide"> <d-title> <h1>the bug that taught me more about PyTorch than years of using it</h1> <p>a loss plateau that looked like my mistake turned out to be a PyTorch bug. tracking it down meant peeling back every layer of abstraction, from optimizer internals to GPU kernels.</p> </d-title> <d-byline></d-byline> <d-article> <p><code class="language-plaintext highlighter-rouge">Expected to fix: my hyperparameters. Actually had to fix: PyTorch backend.</code></p> <p>My training loss plateaued and wouldn‚Äôt budge. Obviously I‚Äôd screwed something up. I tried every hyperparameter combination, rewrote my loss function, spent days assuming I‚Äôd made some stupid mistake. Because it‚Äôs always user error.</p> <p>This time, it wasn‚Äôt. It was a niche PyTorch bug that forced me through layers of abstraction I normally never think about: optimizer internals, memory layouts, dispatch systems, kernel implementations. Taught me more about the framework than years of using it.</p> <p>I had a surprisingly fun time with this bug hunt and wrote up the whole investigation step-by-step, explaining framework internals as they become necessary to crack the case. If you enjoy debugging mysteries or find that tracking down bugs teaches you more than docs ever could, this might resonate. üïµÔ∏è‚Äç‚ôÄÔ∏è</p> <p><em>Debugging post-mortems sometimes make me worry I wouldn‚Äôt have been smart enough to figure them out myself. So I structured this walkthrough to show the reasoning behind each step: why I tested that hypothesis, why that result pointed me in a specific direction. The goal was to make each decision feel like the natural next move, the kind of thing any ML engineer would try with enough curiosity and stubbornness. Background knowledge appears exactly when you need it to understand the next step- think of it as an excuse to learn (or re-learn) PyTorch internals through a real problem.</em></p> <p>If you‚Äôd prefer to jump straight to reproducing the bug yourself, check out the <a href="https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug" rel="external nofollow noopener" target="_blank">minimal reproduction script and walkthrough</a> on GitHub. Otherwise, join me on the investigation!</p> <p><strong>Table of Contents:</strong> ü§î <a href="#the-mystery-a-plateauing-loss">The Mystery: A Plateauing Loss</a>‚Ä¶‚Ä¶ üîé <a href="#isolating-the-problem">Isolating the Problem</a>‚Ä¶‚Ä¶ üíª <a href="#device-specific-differences">Device-Specific Differences</a>‚Ä¶‚Ä¶ ‚å∫ <a href="#tensor-memory-layouts">Tensor Memory Layouts</a>‚Ä¶‚Ä¶ üíî <a href="#identifying-the-broken-operations">Identifying the Broken Operations</a>‚Ä¶‚Ä¶. üçé <a href="#inside-the-kernel-implementation">Inside the Kernel Implementation</a>‚Ä¶‚Ä¶ üïµÔ∏è‚Äç‚ôÄÔ∏è <a href="#case-closed">Case Closed</a></p> <details> <summary><b>TL;DR - Just tell me the bug</b></summary> <div> <p><strong>The Bug:</strong> A PyTorch GPU kernel bug silently failed when writing to non-contiguous memory, causing my model‚Äôs encoder weights to freeze during training on Apple Silicon (MPS backend, PyTorch &lt;2.4).</p> <p><strong>The Technical Details:</strong> PyTorch‚Äôs MPS (Apple Silicon GPU) backend had a kernel bug where <code class="language-plaintext highlighter-rouge">addcmul_</code> and <code class="language-plaintext highlighter-rouge">addcdiv_</code> operations silently fail when writing to non-contiguous output tensors.</p> <p><strong>Why It Caused the Training Plateau:</strong></p> <ul> <li>Encoder weights initialized as transpose of decoder ‚Üí non-contiguous memory layout</li> <li>Adam‚Äôs state tensors inherited this layout (<code class="language-plaintext highlighter-rouge">exp_avg</code> and <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> became non-contiguous)</li> <li>MPS kernels for <code class="language-plaintext highlighter-rouge">addcmul_</code>/<code class="language-plaintext highlighter-rouge">addcdiv_</code> don‚Äôt handle non-contiguous outputs correctly</li> <li>Results computed but written to temporary buffer instead of actual tensor</li> <li>For the non-contiguous encoder‚Äôs Adam parameters, <code class="language-plaintext highlighter-rouge">exp_avg_sq.addcmul_()</code> doesn‚Äôt update ‚Üí value stays zero, then the parameter update via <code class="language-plaintext highlighter-rouge">addcdiv_</code> also fails ‚Üí complete silent freeze</li> </ul> <p><strong>The Fix:</strong></p> <ul> <li> <strong>Adjust your code:</strong> Make weights contiguous at initialization</li> <li> <strong>Upgrade PyTorch:</strong> Upgrade to PyTorch ‚â•2.4 (fixes <code class="language-plaintext highlighter-rouge">addcmul_</code>/<code class="language-plaintext highlighter-rouge">addcdiv_</code>)</li> <li> <strong>(Complete fix) Upgrade your Operating System:</strong> Upgrade to macOS 15+ (native non-contiguous tensor support)</li> </ul> <p><strong>Current Status:</strong> Random operations (<code class="language-plaintext highlighter-rouge">normal_</code>, <code class="language-plaintext highlighter-rouge">uniform_</code>, etc.) still have this bug on macOS &lt; 15 as of PyTorch 2.10 (I submitted a <a href="https://github.com/pytorch/pytorch/pull/165267" rel="external nofollow noopener" target="_blank">PR</a> to fix this). Other MPS operations may be affected.</p> <p><strong>Reproduction:</strong> A minimal reproduction script &amp; walkthrough is available at <a href="https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug" rel="external nofollow noopener" target="_blank">https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug</a>.</p> </div> </details> <h2 id="the-mystery-a-plateauing-loss">The Mystery: A Plateauing Loss</h2> <div class="l-body"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/the_bug_that_taught_me_pytorch_post/loss_plateau-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/loss_plateau-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/loss_plateau-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/the_bug_that_taught_me_pytorch_post/loss_plateau.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Training loss plateaued way too early. This felt like a standard hyperparameter issue‚Äî but I‚Äôd trained this same architecture on similar data with similar hyperparameters countless times and hit much lower losses.</p> <p>Still, I tried everything: varied learning rates, tested different schedules, simplified the loss function. Nothing made a difference.</p> <p>Meanwhile, my actual research sat on hold while I was stuck second-guessing everything: was my code broken? My data corrupted? And the creeping doubt- I‚Äôve been doing ML for years, why can‚Äôt I make a simple two-layer autoencoder train properly?</p> <p>The architecture itself is straightforward: a two-layer sparse autoencoder (encoder ‚Äì&gt; sparse hidden layer ‚Äì&gt; decoder). However, it has some training quirks the <em>could</em> be potential culprits: the hidden layer uses TopK sparsity, where only the k largest activations remain (others are zeroed); the training process includes some manual gradient adjustments (gradient clipping for stability and modifications to decoder weight gradients); there‚Äôs an auxiliary loss term to encourage feature activation.</p> <p>Even though I thought my initial hyperparameters were already well-tested, I tried everything: varied learning rates, tested different schedules, tried different k values and hidden dimensions, adjusted the auxiliary loss coefficients.</p> <p>Nothing made a difference.</p> <p>The model was small enough that I was training on my MacBook (using the Apple Silicon GPU) and simple enough I could actually inspect every parameter. So after the standard checks turned up nothing, I started looking at the weights directly.</p> <p>I visualized the weights at initialization and after the first few training steps. The decoder weights were updating- values shifting, gradients being applied, nothing crazy. <strong>But the encoder weights‚Ä¶ weren‚Äôt updating at all.</strong> No NaNs, no suspicious patterns‚Ä¶ they just‚Ä¶ weren‚Äôt changing. They stayed exactly at their initialized values, down to the last decimal place.</p> <p>Both layers participate in the same forward and backward pass. Why would one update and the other freeze completely?</p> <h2 id="isolating-the-problem">Isolating the Problem</h2> <h3 id="are-gradients-flowing">Are Gradients Flowing?</h3> <p>First check: are gradients even making it back to the encoder? The TopK sparsity should make gradients sparse‚Äîonly the k activated features get gradients through backprop, the rest are zeroed. But maybe I messed up the implementation so that <em>no</em> encoder gradients flow at all? Or the manual gradient adjustments I was making somehow blocked everything?</p> <p>After <code class="language-plaintext highlighter-rouge">loss.backward()</code>, the gradient statistics were:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Encoder</strong></th> <th><strong>Decoder</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Max Grad</strong></td> <td>2.35e6</td> <td>6.64e6</td> </tr> <tr> <td><strong>Sparsity</strong></td> <td>88.5% zeros</td> <td>88.5% zeros</td> </tr> </tbody> </table> <p>The encoder gradients were there- and they were pretty big (as intended for my dataset)! And they were sparse (majority zeros) which was also expected, but there were still plenty of non-zero gradients. So gradients are definitely being calculated.</p> <h3 id="is-it-the-optimizer">Is It the Optimizer?</h3> <p>Since the gradients exist but weights aren‚Äôt updating, the optimizer must be doing something wrong. Testing with manual gradient descent:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Manual SGD update
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">weight</span> <span class="o">-=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span>
<span class="c1"># Encoder weights change! ‚úì
</span>
<span class="c1"># But with Adam...
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="c1"># Encoder weights don't change! ‚úó
</span></code></pre></div></div> <div style=" background: var(--global-card-bg-color); border: 1px solid var(--global-divider-color); border-left: 4px solid var(--global-theme-color); border-radius: 8px; padding: 1.5rem; margin: 2rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08); position: relative; transition: box-shadow 0.3s ease; "> <div style=" position: absolute; top: 6px; right: 10px; font-size: 1.6rem; opacity: 0.75; transform: rotate(8deg); pointer-events: none; ">ü§î</div> <div style=" padding-right: 2.5rem; font-size: 1.25rem; font-weight: 600; line-height: 1.4; color: var(--global-text-color); "> The issue is localized to Adam specifically! But why would Adam fail on the encoder but work perfectly on the decoder? </div> </div> <hr> <h3 id="how-adam-works">How Adam Works</h3> <p>To understand what might be breaking, I need to understand what Adam actually does differently from simple gradient descent.</p> <details open=""> <summary><b>Understanding Adam's Algorithm (click to collapse if familiar)</b></summary> <div> <h3 id="two-problems-with-vanilla-sgd">Two Problems with Vanilla SGD</h3> <p>Standard gradient descent (SGD) updates all parameters the same way:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># SGD: one learning rate for everything
</span><span class="n">param</span> <span class="o">=</span> <span class="n">param</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>
</code></pre></div> </div> <p>This creates two fundamental problems:</p> <ol> <li> <p><strong>Different parameters need different learning rates.</strong> Some parameters might consistently get gradients around 1000 while others get 0.01. With SGD‚Äôs fixed learning rate, you‚Äôre stuck: either you move too slowly on small gradients or you overshoot wildly on large ones.</p> </li> <li> <p><strong>The learning rate needs to change over time.</strong> Early in training, you want big steps to explore the space. Later, you need tiny steps to settle into a minimum. SGD requires manually decaying the learning rate on a schedule.</p> </li> </ol> <h3 id="adams-solution-adaptive-learning-rates-via-gradient-magnitude-tracking">Adam‚Äôs Solution: Adaptive Learning Rates via Gradient Magnitude Tracking</h3> <p>Adam maintains two pieces of state per parameter and uses two hyperparameters to control how these states evolve:</p> <p><strong>State variables</strong> (initialized to zero for each parameter):</p> <ul> <li> <code class="language-plaintext highlighter-rouge">exp_avg</code>: Running average of gradients (first moment)</li> <li> <code class="language-plaintext highlighter-rouge">exp_avg_sq</code>: Running average of squared gradients (second moment)</li> </ul> <p><strong>Hyperparameters</strong> (typically beta_1=0.9, beta_2=0.999):</p> <ul> <li> <code class="language-plaintext highlighter-rouge">beta_1</code>: Decay rate for first moment (momentum)</li> <li> <code class="language-plaintext highlighter-rouge">beta_2</code>: Decay rate for second moment (gradient magnitude history)</li> </ul> <p><strong>Here‚Äôs the simplified algorithm:</strong></p> <p>Initialize state (done once per parameter)</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">exp_avg</span> <span class="o">=</span> <span class="nf">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="nf">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div> </div> <p>Each training step:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Update moments with exponential moving averages
</span><span class="n">exp_avg</span> <span class="o">=</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">exp_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
<span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">beta_2</span> <span class="o">*</span> <span class="n">exp_avg_sq</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta_2</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Update step count
# (It effectively starts at 1 to avoid division by zero in bias correction)
</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Bias correction
</span><span class="n">exp_avg_corrected</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta_1</span><span class="o">**</span><span class="n">step</span><span class="p">)</span>
<span class="n">exp_avg_sq_corrected</span> <span class="o">=</span> <span class="n">exp_avg_sq</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta_2</span><span class="o">**</span><span class="n">step</span><span class="p">)</span>

<span class="c1"># Adaptive parameter update
</span><span class="n">param</span> <span class="o">=</span> <span class="n">param</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">exp_avg_corrected</span> <span class="o">/</span> <span class="p">(</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">exp_avg_sq_corrected</span><span class="p">)</span> <span class="o">+</span> <span class="n">Œµ</span><span class="p">)</span>
</code></pre></div> </div> <p><strong>What Each Moment Does:</strong></p> <ul> <li> <p><strong>First moment (<code class="language-plaintext highlighter-rouge">exp_avg</code>)</strong>: Smooths out noisy gradients by averaging recent directions‚Äîlike momentum in physics. When gradients oscillate (+10, -10, +8, -9‚Ä¶), the positive and negative values cancel out, revealing there‚Äôs no consistent direction. Beta_1=0.9 means ‚Äúkeep 90% of old momentum, add 10% of new gradient.‚Äù This smoothed momentum is what gets multiplied by the learning rate in the parameter update: <code class="language-plaintext highlighter-rouge">lr * exp_avg</code>.</p> </li> <li> <p><strong>Second moment (<code class="language-plaintext highlighter-rouge">exp_avg_sq</code>)</strong>: Tracks typical gradient <strong>magnitude</strong> for each parameter by averaging squared gradients. Squaring removes the +/- sign (both +10 and -10 become 100), preventing cancellation. Beta_2=0.999 means ‚Äúkeep 99.9% of magnitude history, add 0.1% of new squared gradient.‚Äù This magnitude normalizes the momentum-based update: <code class="language-plaintext highlighter-rouge">lr * exp_avg / sqrt(exp_avg_sq)</code>. Parameters with consistently large gradients get their updates scaled down (large denominator), while parameters with small gradients get boosted (small denominator). This is how Adam achieves <strong>adaptive per-parameter learning rates</strong>.</p> </li> <li> <p><strong>Epsilon (<code class="language-plaintext highlighter-rouge">Œµ=1e-8</code>)</strong>: Prevents division by zero.</p> </li> </ul> <p><strong>Bias Correction:</strong></p> <p>Both moments start at zero, causing early estimates to be biased toward zero. The correction factor <code class="language-plaintext highlighter-rouge">(1 - Œ≤**step)</code> provides a large boost early to counteract this, effectively ‚Äúwarming up‚Äù the optimizer over the first ~1000-3000 steps. As training progresses, the correction approaches 1 and has negligible effect.</p> <div class="l-body"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/the_bug_that_taught_me_pytorch_post/bias_correction_early-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/bias_correction_early-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/bias_correction_early-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/the_bug_that_taught_me_pytorch_post/bias_correction_early.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The second moment works similarly. Without correction, <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> would be only 0.1% of gradient¬≤ at step 1, but bias correction restores it to the full value.</p> <p>For a deeper dive into Adam‚Äôs design and intuition, as well as other optimizers that use momentum and adaptive learning rates (RMSprop, AdaGrad, etc.), check out <a href="https://cs231n.github.io/neural-networks-3/#update" rel="external nofollow noopener" target="_blank">Stanford‚Äôs CS231n notes on optimization</a>.</p> </div> </details> <p>Knowing what Adam <em>should</em> be doing, let‚Äôs look at the state it‚Äôs maintaining (those <code class="language-plaintext highlighter-rouge">exp_avg</code> and <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> tensors that track momentum and variance) to see what it‚Äôs <em>actually</em> doing.</p> <h3 id="examining-adams-state">Examining Adam‚Äôs State</h3> <p>For our frozen encoder, the maximum values in each state tensor were:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Encoder</strong></th> <th><strong>Decoder</strong></th> </tr> </thead> <tbody> <tr> <td><strong>exp_avg</strong></td> <td>1.96e+05</td> <td>1.70e+06</td> </tr> <tr> <td><strong>exp_avg_sq</strong></td> <td><span style="display: inline-block; border: 2px solid var(--global-theme-color); border-radius: 4px; padding: 1px 10px; font-weight: bold;">0</span></td> <td>1.18e+11</td> </tr> </tbody> </table> <p>Wait, WHAT?! The encoder‚Äôs <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> is zero despite having momentum accumulated in <code class="language-plaintext highlighter-rouge">exp_avg</code>.</p> <p>This feels mathematically impossible‚Ä¶ The second moment (<code class="language-plaintext highlighter-rouge">exp_avg_sq</code>) is zero despite non-zero gradients. Since <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> stores squared gradients, it should NEVER be zero if gradients are non-zero.</p> <p>And if it truly were zero, we‚Äôd see massive weight updates.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">param_update</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="p">(</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">exp_avg_sq</span><span class="p">)</span> <span class="o">+</span> <span class="n">Œµ</span><span class="p">)</span> 
             <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="mf">1.96e5</span> <span class="o">/</span> <span class="p">(</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
             <span class="o">=</span> <span class="mi">196</span> <span class="o">/</span> <span class="mf">1e-8</span>
             <span class="o">=</span> <span class="mf">1.96e10</span>  <span class="c1"># &lt;-- HUGE!
</span></code></pre></div></div> <p>This would be <strong>huge</strong>! Yet we see NO updates‚Ä¶ this paradox points to a deeper issue.</p> <h3 id="testing-hypotheses">Testing Hypotheses</h3> <h4 id="could-it-be-bias-correction">Could it be bias correction?</h4> <p>Adam uses bias correction to counteract zero initialization. Having previously encountered subtle training issues due to Adam bias initialization bugs, I wondered if the correction might be broken here. <d-footnote>üí°If you haven't been hurt by a bias correction bug before, check out <a href="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for" rel="external nofollow noopener" target="_blank">these</a> <a href="https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation/237282#237282" rel="external nofollow noopener" target="_blank">examples</a> to learn the importance of getting this step right!</d-footnote></p> <p>Recall, the bias correction is simply making our effective beta values dependent on the step index, so if the issue has to do with bias correction, it might have some relation to our beta parameters or step index.</p> <p>I tested with different beta values, at different steps, and even beta_2=0 (which bypasses the exponential average entirely, making <code class="language-plaintext highlighter-rouge">exp_avg_sq = grad**2</code> directly). The encoder‚Äôs <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> still stayed zero, making bias correction seem less likely as a culprit.</p> <p>Plus, <code class="language-plaintext highlighter-rouge">exp_avg</code> updated correctly despite using the same bias correction mechanism. So maybe something else is preventing <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> from updating.</p> <h4 id="is-it-a-precision-issue">Is it a precision issue?</h4> <p>My largest gradients were big (1e6), and squared that‚Äôs 1e12. While that <em>is</em> quite large, it shouldn‚Äôt overflow in float32. However, I‚Äôve also been hurt by precision bugs before<d-footnote>Floating point precision issues have a fun habit of causing silent failures/degradations like this one (where it completes but produces incorrect values). Always worth checking, even when it seems unlikely.</d-footnote>, so I had to try it anyway.</p> <p>I moved everything to float64‚Ä¶ <strong>AND IT STARTED WORKING!</strong></p> <div style=" margin: 2rem 0; padding: 2rem; background: repeating-linear-gradient( 45deg, color-mix(in srgb, var(--global-theme-color) 8%, var(--global-bg-color)), color-mix(in srgb, var(--global-theme-color) 8%, var(--global-bg-color)) 10px, color-mix(in srgb, var(--global-theme-color) 12%, var(--global-bg-color)) 10px, color-mix(in srgb, var(--global-theme-color) 12%, var(--global-bg-color)) 20px ); border: 1px solid color-mix(in srgb, var(--global-theme-color) 20%, transparent); border-radius: 8px; font-family: 'Comic Sans MS', cursive, sans-serif; color: var(--global-text-color); line-height: 1.7; position: relative; overflow: hidden; "> <div style=" position: absolute; top: 10px; right: 15px; font-size: 2rem; opacity: 0.4; transform: rotate(15deg); ">üòµ‚Äçüí´</div> <span style="font-size: 1.2em; font-weight: bold; color: var(--global-theme-color);">Wait... how could this possibly be a precision issue?!</span> <p style="margin: 1rem 0; font-style: italic; color: color-mix(in srgb, var(--global-text-color) 85%, transparent);"> I asked Claude to help me understand the situation &amp; was told there are intermediate calculations in Adam that might overflow...</p> <p style="color: var(--global-text-color);">...but I couldn't find these mysterious intermediates in the code. And how would an overflow produce exact zeros instead of inf/NaN? Maybe we divide by the inf somewhere? Or there's an error correction step? Or we're underflowing? But that shouldn't give ALL zeros?!?! </p> <p style="margin: 1rem 0; font-weight: bold; color: var(--global-theme-color);"> ...Going to fp64 <em>DID</em> fix it though, and LLMs probably know PyTorch better than I do, so maybe I'm missing something obvious? But where was this secret intermediate? I couldn't find it anywhere... </p> <div style="text-align: center; margin-top: 1.5rem; font-size: 1.1em; color: var(--global-theme-color);"> <em>so now what???</em> </div> </div> <p>After a few more minutes of spiraling<d-footnote> You're probably not reading this for the mid-debugging-self-doubt, but every debugging adventure has a spiraling moment (at least for me) so feels disingenuous to skip this step. And maybe one of these theories could've actually been correct! </d-footnote>, I realized something: when I switched to float64, I <em>also</em> had to switch from MPS (Apple Silicon GPU) to CPU, since MPS doesn‚Äôt support float64. <strong>I‚Äôd changed two variables at once.</strong></p> <p>Testing with float32 on CPU‚Ä¶ <strong>the weights update!!</strong></p> <div style=" background: var(--global-card-bg-color); border: 1px solid var(--global-divider-color); border-left: 4px solid var(--global-theme-color); border-radius: 8px; padding: 1.5rem; margin: 2rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08); position: relative; transition: box-shadow 0.3s ease; "> <div style=" position: absolute; top: 6px; right: 10px; font-size: 1.6rem; opacity: 0.75; transform: rotate(8deg); pointer-events: none; ">üí°</div> <div style=" padding-right: 2.5rem; font-size: 1.25rem; font-weight: 600; line-height: 1.4; color: var(--global-text-color); "> Turns out, precision wasn't the culprit, it was <code style="background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;">device-specific</code>! The exact same float32 code updates weights on CPU but fails on MPS. This was progress: same code, same datatypes, but different devices meant different implementations‚Äîand different bugs. </div> </div> <p>Ôπ° This is progress!!</p> <p>Ôπ° Note to self‚Ä¶ simpler explanations are more likely correct- even (and especially!) when LLMs confidently assert complicated theories that are hard to understand / verify</p> <p>Ôπ° Now I just need to figure out why the bug only occurs with MPS</p> <h2 id="device-specific-differences">Device-Specific Differences</h2> <h3 id="why-the-same-operation-behaves-differently-on-different-chips">Why the Same Operation Behaves Differently on Different Chips</h3> <p>PyTorch‚Äôs device abstraction lets you write the same code and run it on CPUs, GPUs, and even Apple Silicon. It <em>feels</em> like the same computation is running everywhere ‚Äî but under the hood, each device has its own entirely separate implementation.</p> <p>When you call a tensor operation like <code class="language-plaintext highlighter-rouge">matmul</code>, PyTorch looks at the tensor‚Äôs metadata (e.g. device, dtype, shape) and dispatches to a <strong>specialized kernel</strong>: a device-specific, highly optimized implementation tailored for that particular hardware backend.</p> <details><summary><b>Understanding Apple's GPU Stack and "Kernel" Terminology</b></summary> <div> <p><strong>Apple‚Äôs GPU Stack:</strong></p> <ul> <li> <strong>Metal</strong> - Apple‚Äôs low-level graphics/compute API (like CUDA for NVIDIA)</li> <li> <strong>MPS (Metal Performance Shaders)</strong> - High-level optimized functions built on Metal (like cuDNN for CUDA)</li> <li> <strong>PyTorch‚Äôs MPS backend</strong> - PyTorch‚Äôs integration that uses both Metal directly and MPS functions</li> </ul> <p><strong>On ‚ÄúKernel‚Äù Terminology:</strong></p> <p>Typically, ‚Äúkernel‚Äù refers to low-level GPU code that runs directly on hardware: functions that explicitly manage parallelism across thousands of GPU cores, handle device memory allocation, and are written in chip-specific languages like CUDA or Metal Shading Language.</p> <p>However, PyTorch seems to also use ‚Äúkernel‚Äù to describe a higher-level abstraction: the framework‚Äôs implementation code (C++, Objective-C++, or CUDA files in the <code class="language-plaintext highlighter-rouge">native/</code> directory) that handles specific operations for specific backends. These PyTorch kernels sit above the hardware level- they might call optimized libraries like MPS or cuDNN (which then use those low-level GPU kernels underneath), or they might contain hand-written GPU code.</p> <p>In this post, we end up primarily exploring PyTorch kernels (e.g. the C++/Objective-C++ code in <code class="language-plaintext highlighter-rouge">BinaryOps.mm</code> that orchestrates MPS operations) rather than the Metal compute shaders executing on GPU cores beneath them.</p> <p>I was surprised these higher-level implementations are also called ‚Äúkernels‚Äù and maybe I have just confused my terminology here but I didn‚Äôt have a better name for them so I tried to mostly use ‚ÄúPyTorch kernel‚Äù or just ‚Äúoperation‚Äù to describe them, though the terminology does get blurry in places.</p> </div> </details> <p>So when you write something like <code class="language-plaintext highlighter-rouge">result = tensor_a @ tensor_b</code>, you‚Äôre not invoking a universal multiply function. PyTorch uses the tensors‚Äô metadata to select a device- and dtype-specific kernel that performs the actual computation.</p> <p>Multiplying two tensors on the CPU uses a completely different kernel than on MPS or CUDA. Even on the same device, changing the dtype or layout can trigger a different kernel. PyTorch maintains a large set of these implementations to support all the combinations.</p> <p>We‚Äôll see exactly how this dispatch system works in C++ later when we dive into the source code. For now, the important point is: <strong><em>even with identical Python code</em> different tensor metadata ‚Üí different kernel code ‚Üí different efficiency / bugs.</strong></p> <p>In my case, because I‚Äôm running this on my M3 MacBook Pro, I‚Äô m using MPS (Metal Performance Shaders), which is the GPU backend for Apple Silicon. While it feels a bit crazy to assume that my training plateau is due to an internal kernel-level bug, it‚Äôs a bit less unreasonable with MPS as it‚Äôs newer and less mature than the CPU and CUDA backends. (And honestly, most people training/debugging ML models are not doing it on their MacBooks.)</p> <h3 id="why-does-only-the-encoder-hit-this-bug">Why Does Only the Encoder Hit This Bug?</h3> <p>The Adam bug appears when working with the encoder on MPS. What makes the encoder different from the decoder that would trigger different behavior?</p> <p>I tested everything I could think of that might differentiate the two tensors:</p> <ul> <li>Different gradient scales</li> <li>Dense vs sparse gradient patterns</li> <li>Removing decoder-specific gradient transformations</li> <li>Making encoder and decoder gradients statistically identical</li> </ul> <p>Nothing helped. Even when both tensors had similar gradient statistics, only the encoder‚Äôs <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> stayed frozen. The difference wasn‚Äôt in the <em>values</em> of the tensor - something else about the encoder tensor itself was triggering the bug.</p> <p><strong>What properties does a PyTorch tensor even have?</strong> I asked Claude what attributes could differ between two tensors and checked them one-by-one:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Encoder</strong></th> <th><strong>Decoder</strong></th> <th><strong>Same?</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Device</strong></td> <td>mps:0</td> <td>mps:0</td> <td>‚úì</td> </tr> <tr> <td><strong>Dtype</strong></td> <td>float32</td> <td>float32</td> <td>‚úì</td> </tr> <tr> <td><strong>Shape</strong></td> <td>[384, 1536]</td> <td>[1536, 384]</td> <td>‚úì</td> </tr> <tr> <td><strong>Requires_grad</strong></td> <td>True</td> <td>True</td> <td>‚úì</td> </tr> <tr> <td><strong>Stride</strong></td> <td>(1536, 1)</td> <td>(384, 1)</td> <td>‚ùå</td> </tr> <tr> <td><strong>Contiguous</strong></td> <td>False</td> <td>True</td> <td>‚ùå</td> </tr> </tbody> </table> <p>Two differences! The encoder has a different stride pattern and is non-contiguous (these are related - more on that below). Maybe the MPS Adam bug only affects non-contiguous tensors? Worth a shot:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="c1"># Encoder updates!! ‚úì
</span></code></pre></div></div> <p><strong>IT WORKS!</strong> But <em>why</em>?</p> <h2 id="tensor-memory-layouts">Tensor Memory Layouts</h2> <h3 id="what-does-contiguous-even-mean">What Does ‚ÄúContiguous‚Äù Even Mean?</h3> <p>Your computer‚Äôs memory is just a flat, 1D array of bytes, but tensors represent multi-dimensional grids. When you index <code class="language-plaintext highlighter-rouge">tensor[i, j]</code>, PyTorch needs to find that element in the flat memory. The tensor‚Äôs <strong>stride</strong> tells it how to do this conversion (and the exact amount you jump between elements depends on the dtype and how much memory each element takes up).</p> <p>Think of stride as <strong>navigation instructions</strong>: ‚Äúto get from one row to the next, skip this many elements.‚Äù By default, memory is stored row-wise‚Äîeach row is stored sequentially, then the next row comes after. If you read through a row, you skip over 1 element at a time; to go to the next row, you move row-length elements over. (This is why going across a row is faster than going down a column.)</p> <p>However, the memory layout doesn‚Äôt have to match the logical layout we use to think about the tensor. We can change how the user views the tensor without moving any data! For example, when we run transpose (<code class="language-plaintext highlighter-rouge">.T</code>), we don‚Äôt need to move around any data‚Äîwe just change the stride!</p> <div class="l-body"> <div class="row"> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/the_bug_that_taught_me_pytorch_post/memory_layout_contig-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/memory_layout_contig-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/memory_layout_contig-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/the_bug_that_taught_me_pytorch_post/memory_layout_contig.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/the_bug_that_taught_me_pytorch_post/memory_layout_non_contig-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/memory_layout_non_contig-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/memory_layout_non_contig-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/the_bug_that_taught_me_pytorch_post/memory_layout_non_contig.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>As we see in the images, reading all the elements row-by-row in the contiguous tensor is easy and linear, but the same row-wise pattern in the non-contiguous tensor is much jumpier. This jumping pattern makes the tensor ‚Äúnon-contiguous.‚Äù</p> <p>While there‚Äôs only one way for a tensor to be contiguous (the ‚Äúnatural‚Äù layout), there are many ways to become non-contiguous. By default, tensors are initialized as contiguous, but operations like slicing (<code class="language-plaintext highlighter-rouge">tensor[::2, :]</code>), reshaping, and dimension reordering (<code class="language-plaintext highlighter-rouge">permute</code>) can all create different non-contiguous stride patterns.</p> <p><strong>Why design tensors this way?</strong> Wouldn‚Äôt it be simpler to always keep data in the ‚Äúnatural‚Äù contiguous layout? The answer is performance: by just adjusting the tensor‚Äôs metadata, operations like transpose, slice, and reshape can be nearly <strong>instant</strong>‚Äî no data movement or memory allocation required. Keeping everything contiguous would mean expensive copying every time you reorganize dimensions.</p> <h3 id="how-my-encoder-became-non-contiguous">How My Encoder Became Non-Contiguous</h3> <p>Looking at the weight initialization code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">.T</code> creates a non-contiguous view, and <code class="language-plaintext highlighter-rouge">.clone()</code> preserves the stride pattern.</p> <details> <summary><b>Why does <code>.clone()</code> preserve stride patterns?</b></summary> <div> <p>At first this felt counterintuitive to me- if we‚Äôre already paying the cost to copy the data (the whole point of non-contiguous layouts is to avoid copying), why not copy it into the ‚Äúbetter‚Äù contiguous layout?</p> <p>But this actually makes sense from a design perspective: <code class="language-plaintext highlighter-rouge">.clone()</code> should create an exact copy with all properties preserved, including memory layout. The tensor might be non-contiguous for a reason‚Äîmaybe you‚Äôre about to transpose it back, or the layout is optimized for some operation. Silently reorganizing memory would be surprising behavior. (The optional <a href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.memory_format</code></a> argument, which defaults to <code class="language-plaintext highlighter-rouge">torch.preserve_format</code>, makes this choice explicit.)</p> <p>As a bonus, preserving the layout is also faster. Even though both include new memory allocation and moving data, reorganizing it still slows things down:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">x_t</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">T</span>  <span class="c1"># Start with non-contiguous
</span><span class="n">y_noncontig</span> <span class="o">=</span> <span class="n">x_t</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>              <span class="c1"># Preserves non-contiguous (1.919ms)
</span><span class="n">y_contig</span> <span class="o">=</span> <span class="n">x_t</span><span class="p">.</span><span class="nf">clone</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">contiguous_format</span><span class="p">)</span>  <span class="c1"># Force contiguous (4.401ms)
</span></code></pre></div> </div> </div> </details> <p><strong>Okay so we now know this initialization is why only the encoder is non-contiguous, and thus why only the encoder has training issues!</strong></p> <p><em>While I could just call <code class="language-plaintext highlighter-rouge">.contiguous()</code> on my encoder, declare victory, and get back to the research this bug was blocking me from doing‚Ä¶ I felt like I was just scratching the surface of this bug and I feared it would haunt me until I fully figured out WHAT happened and WHY.</em></p> <div style=" background: var(--global-card-bg-color); border: 1px solid var(--global-divider-color); border-left: 4px solid var(--global-theme-color); border-radius: 8px; padding: 1.5rem; margin: 2rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08); position: relative; transition: box-shadow 0.3s ease; "> <div style=" position: absolute; top: 6px; right: 10px; font-size: 1.6rem; opacity: 0.75; transform: rotate(8deg); pointer-events: none; ">üîé</div> <div style=" padding-right: 2.5rem; font-size: 1.25rem; font-weight: 600; line-height: 1.4; color: var(--global-text-color); "> Why does a non-contiguous encoder weight cause zero second moment and no parameter updates with Adam on MPS?? </div> </div> <h2 id="identifying-the-broken-operations">Identifying the Broken Operations</h2> <h3 id="what-operations-does-adam-use">What Operations Does Adam Use?</h3> <p>When Adam updates parameters, what operations does it perform? Let‚Äôs look at <a href="https://github.com/pytorch/pytorch/blob/main/torch/optim/adam.py" rel="external nofollow noopener" target="_blank">PyTorch‚Äôs Adam implementation</a>.</p> <p>Fair warning: this file is over 1000 lines! To find what we need, search for where <code class="language-plaintext highlighter-rouge">exp_avg</code> and <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> are defined and updated.</p> <p>Here are the critical lines (<a href="https://github.com/pytorch/pytorch/blob/39901f229520a5256505ec24782f716ee7ddc843/torch/optim/adam.py#L101" rel="external nofollow noopener" target="_blank">lines 101, 391-407</a>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># State initialization (line 101)
</span><span class="n">state</span><span class="p">[</span><span class="sh">"</span><span class="s">exp_avg</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">preserve_format</span><span class="p">)</span>
<span class="n">state</span><span class="p">[</span><span class="sh">"</span><span class="s">exp_avg_sq</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">preserve_format</span><span class="p">)</span>

<span class="c1"># ... [300 lines of setup and parameter group handling] ...
</span>
<span class="c1"># First moment update (line 391)
</span><span class="n">exp_avg</span><span class="p">.</span><span class="nf">lerp_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

<span class="c1"># Second moment update (line 392)
</span><span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">).</span><span class="nf">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

<span class="c1"># ... [bias correction calculations] ...
</span>
<span class="c1"># Parameter update (line 407)
</span><span class="n">param</span><span class="p">.</span><span class="nf">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>
</code></pre></div></div> <p>Look at that initialization! <code class="language-plaintext highlighter-rouge">memory_format=torch.preserve_format</code> means the state tensors inherit their stride pattern from <code class="language-plaintext highlighter-rouge">param</code>. So when our encoder weight is non-contiguous, both <code class="language-plaintext highlighter-rouge">exp_avg</code> and <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> are also non-contiguous.</p> <p>But they‚Äôre BOTH non-contiguous - so why does only one break?</p> <p>Well, while they both are computed via addition and multiplication, they don‚Äôt use the exact same operations to perform this. Any of these operations could be a suspect, so let‚Äôs test each one individually!</p> <p>For operations like <code class="language-plaintext highlighter-rouge">output.addcmul_(input1, input2)</code>, the <strong>output tensor</strong><d-footnote>In PyTorch, when a function name ends with an underscore (like <code>mul_</code>), that indicates that it is performing an <b>in-place operation</b> to modify a tensor directly in memory. Just as different devices can distinct kernels, so can distinctions like these!</d-footnote> is modified while <strong>input tensors</strong> are read from. In our case, we know the output tensor is non-contiguous, so let‚Äôs test if that is sufficient to cause our bug.</p> <h3 id="testing-the-broken-operations">Testing the Broken Operations</h3> <p>Testing each Adam operation with non-contiguous output tensors on MPS:</p> <table> <thead> <tr> <th><strong>Operation</strong></th> <th><strong>Function</strong></th> <th><strong>Result</strong></th> </tr> </thead> <tbody> <tr> <td>Linear interpolation</td> <td><code class="language-plaintext highlighter-rouge">lerp_()</code></td> <td>Updates ‚úì</td> </tr> <tr> <td>Scalar multiply</td> <td><code class="language-plaintext highlighter-rouge">mul_()</code></td> <td>Updates ‚úì</td> </tr> <tr> <td>Add + multiply</td> <td><code class="language-plaintext highlighter-rouge">addcmul_()</code></td> <td>Stays zero ‚úó</td> </tr> <tr> <td>Add + divide</td> <td><code class="language-plaintext highlighter-rouge">addcdiv_()</code></td> <td>Stays zero ‚úó</td> </tr> </tbody> </table> <div style=" background: var(--global-card-bg-color); border: 1px solid var(--global-divider-color); border-left: 4px solid var(--global-theme-color); border-radius: 8px; padding: 1.5rem; margin: 2rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08); position: relative; transition: box-shadow 0.3s ease; "> <div style=" position: absolute; top: 6px; right: 10px; font-size: 1.6rem; opacity: 0.75; transform: rotate(8deg); pointer-events: none; ">‚ÄºÔ∏è</div> <div style=" padding-right: 2.5rem; font-size: 1.25rem; font-weight: 600; line-height: 1.4; color: var(--global-text-color); "> Found it! <code style="background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;">addcmul_()</code> and <code style="background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;">addcdiv_()</code> both fail silently when writing to non-contiguous outputs on MPS. </div> </div> <p>Interestingly, <em>input contiguity doesn‚Äôt matter</em>, only the output! Whether <code class="language-plaintext highlighter-rouge">grad</code>, <code class="language-plaintext highlighter-rouge">exp_avg</code>, or <code class="language-plaintext highlighter-rouge">denom</code> are contiguous makes no difference. The bug is purely in how these kernels write to <em>non-contiguous output buffers</em>.</p> <p>The broken operations aren‚Äôt producing zeros or NaNs. They‚Äôre simply not modifying the output tensor at all. This wasn‚Äôt immediately obvious since <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> was initialized to zeros, making ‚Äústays at zero‚Äù and ‚Äúnever updates‚Äù look identical. But testing with a non-zero, non-contiguous output tensor confirms that after calling <code class="language-plaintext highlighter-rouge">addcmul_</code> or <code class="language-plaintext highlighter-rouge">addcdiv_</code>, the values remain unchanged. No update happens.</p> <p>Yet timing shows MPS <em>is</em> doing substantial work. Non-contiguous operations take &gt;2x longer than contiguous ones, proving the kernels are computing <em>something</em>, yet those results never make it to the output tensor. On CPU, each of these operations work correctly regardless of memory layout. This is purely a MPS-specific bug.</p> <p>With the broken operations identified, we can trace the complete chain of events that triggers our failure:</p> <h3 id="putting-the-pieces-together">Putting the Pieces Together</h3> <div class="l-body"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/the_bug_that_taught_me_pytorch_post/complete_bug_chain-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/complete_bug_chain-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/complete_bug_chain-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/the_bug_that_taught_me_pytorch_post/complete_bug_chain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <details> <summary><b>Show the complete bug chain in code</b></summary> <div> <p><strong>Step 1: Initialization</strong></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Creates non-contiguous encoder weight (stride: 1, 1536)
</span><span class="n">encoder</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
</code></pre></div> </div> <p><strong>Step 2: Adam State Creation</strong></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Both state tensors inherit non-contiguous layout from param
</span><span class="n">state</span><span class="p">[</span><span class="sh">"</span><span class="s">exp_avg</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="nf">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">preserve_format</span><span class="p">)</span>
<span class="n">state</span><span class="p">[</span><span class="sh">"</span><span class="s">exp_avg_sq</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="nf">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">preserve_format</span><span class="p">)</span>
</code></pre></div> </div> <p><strong>Step 3: Optimization Loop</strong></p> <p><em>First moment update:</em></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">exp_avg</span><span class="p">.</span><span class="nf">lerp_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">beta_1</span><span class="p">)</span>  <span class="c1"># ‚úì Works fine
</span></code></pre></div> </div> <p><em>Second moment update:</em></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">mul_</span><span class="p">(</span><span class="n">beta_2</span><span class="p">)</span>                        <span class="c1"># ‚úì Works fine
</span><span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">beta_2</span><span class="p">)</span>      <span class="c1"># ‚úó No update - stays zero!
</span></code></pre></div> </div> <p><strong>Step 4: Parameter Update</strong></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Should update param, does nothing, leading to silent failure
</span><span class="n">param</span><span class="p">.</span><span class="nf">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>  <span class="c1"># ‚úó No update!
</span></code></pre></div> </div> </div> </details> <p>If <em>only</em> <code class="language-plaintext highlighter-rouge">exp_avg_sq.addcmul_()</code> failed, the zero <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> would produce massive weight explosions (update = <code class="language-plaintext highlighter-rouge">lr √ó exp_avg / ‚àö(Œµ)</code>), making the bug immediately obvious. But <code class="language-plaintext highlighter-rouge">param.addcdiv_()</code> <em>also</em> failed, producing no updates at all!</p> <p>The second bug masked the first, creating a silent failure: the spookiest type of error. The model appeared to be learning (the decoder was training normally), but progress stalled because the encoder stayed frozen. A subtle plateau that looked exactly like a hyperparameter issue üôÉ</p> <details> <summary><b>Side note: Why did forward and backward passes work fine with non-contiguous weights?</b></summary> <div> <p>If non-contiguous tensors can cause operations to silently fail on MPS, why didn‚Äôt the forward pass or backward pass break?</p> <p>The forward and backward passes for <code class="language-plaintext highlighter-rouge">F.linear</code> use <code class="language-plaintext highlighter-rouge">matmul</code> for their matrix multiplications, which handle non-contiguous tensors correctly on MPS. Testing confirms that both <code class="language-plaintext highlighter-rouge">matmul</code> (the <code class="language-plaintext highlighter-rouge">@</code> operator) and <code class="language-plaintext highlighter-rouge">F.linear</code> work correctly with non-contiguous input tensors and non-contiguous weight matrices on MPS, including during the backward pass where gradients flow through non-contiguous weights without issues.</p> <p>The bug is specific to the fused in-place operations that Adam uses for state updates: <code class="language-plaintext highlighter-rouge">addcmul_</code> and <code class="language-plaintext highlighter-rouge">addcdiv_</code>. These operations fail silently when writing to non-contiguous output tensors, while other in-place operations like <code class="language-plaintext highlighter-rouge">lerp_</code> and <code class="language-plaintext highlighter-rouge">mul_</code> work correctly.</p> </div> </details> <p><strong>While we have made so much progress on this case, we‚Äôre still not done yet!!</strong></p> <div style=" background: var(--global-card-bg-color); border: 1px solid var(--global-divider-color); border-left: 4px solid var(--global-theme-color); border-radius: 8px; padding: 1.5rem; margin: 2rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08); position: relative; transition: box-shadow 0.3s ease; "> <div style=" position: absolute; top: -12px; left: 1rem; background: var(--global-theme-color); color: var(--global-hover-text-color); padding: 0.4rem 1rem; border-radius: 1rem; font-size: 0.75rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.5px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); ">Remaining Question</div> <div style=" position: absolute; top: 6px; right: 10px; font-size: 1.6rem; opacity: 0.75; transform: rotate(8deg); pointer-events: none; ">üîç</div> <div style=" margin-top: 0.5rem; padding-right: 2.5rem; font-size: 1.25rem; font-weight: 600; line-height: 1.4; color: var(--global-text-color); "> Why do <code style="background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;">addcmul_</code> and <code style="background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;">addcdiv_</code> fail to update non-contiguous outputs while <code style="background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;">mul_</code> and <code style="background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;">lerp_</code> work fine? </div> </div> <h2 id="inside-the-kernel-implementation">Inside the Kernel Implementation</h2> <p>To understand why some operations work and others don‚Äôt, I needed to look at PyTorch‚Äôs source code for the buggy kernels.</p> <p>While I normally trace through a Python codebase by jumping to definitions in my IDE, that doesn‚Äôt work with <code class="language-plaintext highlighter-rouge">tensor.addcmul_()</code>. When you call this function, there‚Äôs no Python source code executing - instead, Python immediately jumps into compiled C++ code for performance. And since PyTorch ships this as a pre-compiled binary, I can‚Äôt see that C++ implementation.</p> <details> <summary><b>How can Python call C++ functions? (a brief aside on bindings)</b></summary> <div> <p>How can a Python tensor object have methods that execute C++ code? I skipped over this earlier but even though I know PyTorch isn‚Äôt the only framework to do this and everything is just machine code if you zoom in close enough‚Ä¶ it still feels a bit magical to casually call another language.</p> <p>The explanation is <strong>Python bindings</strong>.</p> <p>When you install PyTorch, you‚Äôre not just getting Python files. You‚Äôre also getting compiled C++ libraries (.so files on Linux/Mac, .dll on Windows) that contain the actual mathematical operations. The Python part is essentially a wrapper that:</p> <ol> <li>Takes your Python arguments (<code class="language-plaintext highlighter-rouge">tensor</code>, <code class="language-plaintext highlighter-rouge">other_tensor</code>, etc.)</li> <li>Converts them to C++ data structures</li> <li>Calls the appropriate C++ function</li> <li>Converts the C++ result back to a Python tensor</li> <li>Returns it to your Python code</li> </ol> <p>PyTorch uses <a href="https://pybind11.readthedocs.io/" rel="external nofollow noopener" target="_blank">pybind11</a> to automatically generate this wrapper code. For example, the C++ function signature:</p> <div class="language-cpp highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">addcmul_</span><span class="p">(</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">tensor1</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">tensor2</span><span class="p">,</span> <span class="k">const</span> <span class="n">Scalar</span><span class="o">&amp;</span> <span class="n">value</span><span class="p">)</span>
</code></pre></div> </div> <p>Gets automatically wrapped so you can call it from Python as:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">tensor</span><span class="p">.</span><span class="nf">addcmul_</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div> </div> <p>This is why PyTorch operations are fast despite being called from Python - the heavy lifting happens in optimized C++ code, with Python just handling the interface.</p> </div> </details> <p>And as we discussed earlier, PyTorch dispatches based on tensor metadata, so there isn‚Äôt just <em>one</em> implementation - there are device-specific kernels for CPU, CUDA, MPS, etc. Since my PyTorch installation just has the compiled binary files, to investigate the actual implementations, we need to clone PyTorch‚Äôs repository.</p> <h3 id="pytorchs-dispatch-system">PyTorch‚Äôs Dispatch System</h3> <p>All kernels are listed in an <strong>operation registry</strong> - a YAML file that maps operation names (like <code class="language-plaintext highlighter-rouge">addcmul_</code>) to their tensor-specific C++ implementations. In practice, when PyTorch is compiled (normally done before you install it), this registry is used to automatically generate hundreds of scripts that do the actual dispatching based on the patterns described here, but if we just want to understand what kernel our tensor is calling, we can look through the registry.</p> <p>Searching for ‚Äúaddcmul_‚Äù in the registry <code class="language-plaintext highlighter-rouge">native_functions.yaml</code>:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">func</span><span class="pi">:</span> <span class="s">addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor(a!)</span>
  <span class="c1"># our addcmul_ function just points us to the yaml for addcmul.out</span>
  <span class="na">structured_delegate</span><span class="pi">:</span> <span class="s">addcmul.out</span>

<span class="c1"># The function addcmul_ points to:</span>
<span class="pi">-</span> <span class="na">func</span><span class="pi">:</span> <span class="s">addcmul.out(...)</span>
  <span class="na">dispatch</span><span class="pi">:</span>
    <span class="s">CPU, CUDA</span><span class="err">:</span> <span class="s">addcmul_out</span>
    <span class="s">MPS</span><span class="err">:</span> <span class="s">addcmul_out_mps</span>  <span class="c1"># Different function for MPS!</span>
</code></pre></div></div> <p>Now that we have the device-specific operation names, we can search them in the PyTorch repo within the <a href="`https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/mps/`">mps implementations</a>, and we find our implementation for <code class="language-plaintext highlighter-rouge">addcmul_out_mps</code> in <a href="https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/mps/operations/PointwiseOps.mm" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">PointwiseOps.mm</code></a>. Upon a first skim of the code, I realized I had no clue how to read the MPS codebase. There were too many unknown variables and constructs, and I wasn‚Äôt sure what to look for in this implementation. I‚Äôd written a CUDA kernel before, and was pretty good with C about a decade ago, but as turns out, neither of those helped here :(</p> <h3 id="comparing-broken-vs-working-implementations">Comparing Broken vs Working Implementations</h3> <p>Rather than trying to decode unfamiliar code in isolation, I‚Äôd find something similar that works correctly and compare the two. <code class="language-plaintext highlighter-rouge">mul_</code> was the perfect comparison since both are simple element-wise in-place operations. The registry pointed me to <code class="language-plaintext highlighter-rouge">binaryOpTensor</code> in <a href="https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/mps/operations/BinaryOps.mm" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">BinaryOps.mm</code></a>.</p> <p>Now I had my comparison:</p> <ul> <li> <strong>Broken:</strong> <code class="language-plaintext highlighter-rouge">addc_mul_div_out_mps</code> in <code class="language-plaintext highlighter-rouge">PointwiseOps.mm</code> (used by <code class="language-plaintext highlighter-rouge">addcmul_</code>)</li> <li> <strong>Working:</strong> <code class="language-plaintext highlighter-rouge">binaryOpTensor</code> in <code class="language-plaintext highlighter-rouge">BinaryOps.mm</code> (used by <code class="language-plaintext highlighter-rouge">mul_</code>)</li> </ul> <p>I opened both side-by-side, scanning specifically for differences in how they handle the output tensor. My experiments had already narrowed the search: I knew both operations were computing <em>something</em> (timing proved that), so the bug had to be in how results get written back to non-contiguous outputs. Look for anything related to contiguity checks or special output handling.</p> <p><strong>Broken version (<code class="language-plaintext highlighter-rouge">addcmul_</code>):</strong></p> <div class="language-objc highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">void</span> <span class="nf">addc_mul_div_out_mps</span><span class="p">(...,</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">output</span><span class="p">,</span> <span class="p">...)</span> <span class="p">{</span>
  <span class="c1">// ... setup code ...</span>
  <span class="n">Placeholder</span> <span class="n">outputPlaceholder</span> <span class="o">=</span> <span class="n">Placeholder</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>
  <span class="n">runMPSGraph</span><span class="p">(...);</span>
  <span class="c1">// That's it - no additional handling</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Working version (<code class="language-plaintext highlighter-rouge">mul_</code>):</strong></p> <div class="language-objc highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">void</span> <span class="nf">binaryOpTensor</span><span class="p">(...,</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">output</span><span class="p">,</span> <span class="p">...)</span> <span class="p">{</span>
  <span class="c1">// ... setup code ...</span>
  
  <span class="n">bool</span> <span class="n">needsCopyToOutput</span> <span class="o">=</span> <span class="o">!</span><span class="n">output</span><span class="p">.</span><span class="n">is_contiguous</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">needsCopyToOutput</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Create temporary contiguous tensor</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">empty</span><span class="p">(...);</span>
  <span class="p">}</span>
  
  <span class="n">Placeholder</span> <span class="n">outputPlaceholder</span> <span class="o">=</span> <span class="n">Placeholder</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>
  <span class="n">runMPSGraph</span><span class="p">(...);</span>
  
  <span class="k">if</span> <span class="p">(</span><span class="n">needsCopyToOutput</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">output_</span><span class="p">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>  <span class="c1">// Copy results back!</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>The working version explicitly checks <code class="language-plaintext highlighter-rouge">!output.is_contiguous()</code> and adds extra handling: it creates a temporary contiguous tensor, runs the operation, then copies results back. The broken version just passes the output directly to <code class="language-plaintext highlighter-rouge">Placeholder</code> and calls it a day.</p> <p>But this raises a new question: if non-contiguous memory layouts need this kind of explicit handling, why doesn‚Äôt <code class="language-plaintext highlighter-rouge">addcmul</code> just crash or throw an error instead of silently failing?</p> <h3 id="the-memory-conversion-problem">The Memory Conversion Problem</h3> <p>The answer lies in understanding what <code class="language-plaintext highlighter-rouge">Placeholder</code> does. PyTorch tensors and Metal (Apple‚Äôs GPU framework) use different memory formats, so PyTorch needs a converter when running operations on Apple Silicon. <code class="language-plaintext highlighter-rouge">Placeholder</code> handles this conversion - it takes PyTorch tensors and wraps them in Metal-compatible buffers, handles different data types, manages memory layouts, and sets up the compute pipeline.</p> <p>For most tensors, this conversion is straightforward. But for non-contiguous tensors, Metal can‚Äôt work with the scattered memory layout directly. Looking at the Placeholder code:</p> <div class="language-objc highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">src</span><span class="p">.</span><span class="n">is_contiguous</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">_tensor</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="n">clone</span><span class="p">(</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">Contiguous</span><span class="p">);</span>  <span class="c1">// Create contiguous copy</span>
    <span class="n">srcBuf</span> <span class="o">=</span> <span class="n">getMTLBufferStorage</span><span class="p">(</span><span class="n">_tensor</span><span class="p">);</span>          <span class="c1">// Point Metal to the copy</span>
<span class="p">}</span>
</code></pre></div></div> <p>When Placeholder encounters a non-contiguous tensor, it automatically creates a contiguous copy and points Metal to that copy instead. This happens transparently - the broken kernels have no idea they‚Äôre working with a temporary.</p> <p>This automatic copying is perfect for <strong>input tensors</strong> - Metal reads from the copy, computation proceeds normally, and nobody cares what happens to the temporary afterward.</p> <p>But it‚Äôs disastrous for <strong>output tensors</strong> where the goal is in-place editing. The computation succeeds and writes results to the temporary copy, but those results never make it back to the original tensor that‚Äôs supposed to be updated.</p> <details> <summary><b>Why is this MPS-Specific?</b></summary> <div> <p>If non-contiguous tensors are so problematic, why do CPU and CUDA backends handle them fine?</p> <p><strong>CPU:</strong> Can handle arbitrary strides natively. When iterating through a non-contiguous tensor, the CPU just follows the stride pattern‚Äîjumping around memory is slower than sequential access, but it works correctly.</p> <p><strong>CUDA:</strong> NVIDIA‚Äôs CUDA framework has always supported strided memory access in kernels. Operations can read/write to non-contiguous layouts directly, though with some performance penalty.</p> <p><strong>MPS:</strong> Apple‚Äôs Metal Performance Shaders framework initially didn‚Äôt support strided access. Kernels expected contiguous memory layouts, period. This forced PyTorch to implement the gather-scatter workaround pattern we saw in the working kernels.</p> <p>The bug occurred because some MPS operations implemented this workaround (like <code class="language-plaintext highlighter-rouge">mul_</code>), while others didn‚Äôt (like <code class="language-plaintext highlighter-rouge">addcmul_</code>). The abstraction (Placeholder) that was supposed to hide this complexity actually made it worse by silently copying outputs without a way to copy results back. Although as we‚Äôll learn later this has been improved in newer Mac Operating Systems.</p> </div> </details> <h3 id="the-complete-bug-mechanism">The Complete Bug Mechanism</h3> <div class="l-body"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/the_bug_that_taught_me_pytorch_post/placeholder_bug_mechanism-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/placeholder_bug_mechanism-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/placeholder_bug_mechanism-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/the_bug_that_taught_me_pytorch_post/placeholder_bug_mechanism.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The broken kernels work perfectly with contiguous tensors and silently fail with non-contiguous ones. The working kernels detect this situation and add an explicit copy-back step to move results from the temporary to the original tensor.</p> <h3 id="the-fix">The Fix</h3> <p>Understanding the bug made the solution clear - apply the same pattern that working kernels use:</p> <style>.diff-container{background:#f8f9fa;border-radius:8px;overflow-x:auto;border:1px solid #e0e0e0;max-width:100%;margin:20px 0}.diff-line{display:flex;font-family:'Monaco','Menlo','Ubuntu Mono',monospace;font-size:13px;line-height:1.6;border-bottom:1px solid #e8e8e8}.diff-line:last-child{border-bottom:0}.line-number{padding:4px 12px;background:#f0f0f0;color:#999;text-align:right;user-select:none;min-width:50px;border-right:1px solid #e0e0e0}.line-content{padding:4px 12px;flex:1;white-space:pre}.added{background:#e6ffed;color:#24292e}.added .line-number{background:#cdffd8;color:#22863a}.removed{background:#ffeef0;color:#24292e}.removed .line-number{background:#ffdce0;color:#d73a49}.unchanged{background:white}</style> <div class="diff-container"> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content">Tensor output = output_;</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content">bool needsCopyToOutput = false;</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content"> </div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content">if (!output_.is_contiguous()) {</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content"> output = at::empty(...); // Create contiguous buffer WE manage</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content"> needsCopyToOutput = true;</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content">}</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content"> </div> </div> <div class="diff-line unchanged"> <div class="line-number">1</div> <div class="line-content">@autoreleasepool {</div> </div> <div class="diff-line unchanged"> <div class="line-number">2</div> <div class="line-content"> Placeholder outputPlaceholder = Placeholder(output);</div> </div> <div class="diff-line unchanged"> <div class="line-number">3</div> <div class="line-content"> runMPSGraph(...);</div> </div> <div class="diff-line unchanged"> <div class="line-number">4</div> <div class="line-content">}</div> </div> <div class="diff-line removed"> <div class="line-number">-</div> <div class="line-content">// No copy-back - results vanish when Placeholder dies</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content"> </div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content">if (needsCopyToOutput) {</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content"> output_.copy_(output); // Copy results back</div> </div> <div class="diff-line added"> <div class="line-number">+</div> <div class="line-content">}</div> </div> </div> <p>I tested this locally and it worked! The encoder weights finally updated and the model trained successfully üéâüéâ</p> <p>You can see the complete reproduction, debugging experiments, fix at <a href="https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug" rel="external nofollow noopener" target="_blank">https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug</a>.</p> <h2 id="case-closed">Case Closed</h2> <h3 id="a-lesson-in-version-control">A Lesson in Version Control</h3> <p>While editing a Python package just involves installing your locally editable version of the code instead of the default package, to test my PyTorch fix, I had to re-build it all locally, which was more work than expected and <em>also</em> made me acutely aware that this whole time I was working on PyTorch v2.2.1<d-footnote>I was working on a research codebase with dependency conflicts that blocked upgrading PyTorch. Common enough situation, but lesson learned: always check versions early in debugging, even if you can't immediately update!</d-footnote> (as this fact made it difficult to build and I had to downgrade things like CMake and deal with weird version conflicts to even build this older PyTorch).</p> <p>Checking the latest version revealed the bug was already fixed in v2.4, patched by an ML engineer at Apple last year using almost the exact same approach I‚Äôd used.<d-footnote>The official fix uses slightly different syntax; but the same core pattern: detect non-contiguous output, create a contiguous temporary buffer, perform the computation, then copy results back to the original tensor.</d-footnote> This updated code even informed me that in macOS 15+, MPS now handles non-contiguous tensors natively! <d-footnote>In macOS 15, Apple added native strided array support to MPSGraph via the <code>arrayView</code> API (see <a href="https://developer.apple.com/videos/play/wwdc2024/10218/" rel="external nofollow noopener" target="_blank">WWDC 2024 session</a> at timestamp 13:41). Instead of the gather-scatter workaround, Metal can now read/write directly from non-contiguous memory using stride metadata. This means on macOS 15+, PyTorch can skip the manual copy workarounds entirely. The performance gap between contiguous and non-contiguous tensors is now much smaller, though contiguous is still faster due to better cache utilization.</d-footnote></p> <div style=" background: var(--global-card-bg-color); border: 1px solid var(--global-divider-color); border-left: 4px solid var(--global-theme-color); border-radius: 8px; padding: 1.5rem; margin: 2rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08); position: relative; transition: box-shadow 0.3s ease; "> <div style=" position: absolute; top: 6px; right: 10px; font-size: 1.6rem; opacity: 0.75; transform: rotate(8deg); pointer-events: none; ">ü§¶‚Äç‚ôÄÔ∏è</div> <div style=" padding-right: 2.5rem; font-size: 1.25rem; font-weight: 600; line-height: 1.4; color: var(--global-text-color); "> While I now felt silly for diving so deep on an already-fixed bug, the process was still very fun, educational, and so worth the effort.<br><br>In hindsight, I maybe could've tried upgrading PyTorch earlier...<br><br> ...But as it turns out, <code style="background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;">the story wasn't over just yet!</code> </div> </div> <h3 id="the-pattern-strikes-again">The Pattern Strikes Again</h3> <p>While writing this up, I added some more tests for my kernel fix to confirm it really worked, and one of the tests failed! I looked into it more and realized I‚Äôd stumbled upon <strong>the same failure pattern</strong> in the <code class="language-plaintext highlighter-rouge">random_</code> operation (in the most up-to-date PyTorch this time!)</p> <p><strong>Turns out, all random in-place operations</strong> (<code class="language-plaintext highlighter-rouge">normal_</code>, <code class="language-plaintext highlighter-rouge">uniform_</code>, <code class="language-plaintext highlighter-rouge">exponential_</code>, <code class="language-plaintext highlighter-rouge">random_</code>, <code class="language-plaintext highlighter-rouge">bernoulli_</code>) <strong>silently fail when called on non-contiguous tensors on MPS</strong>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">).</span><span class="n">T</span>  <span class="c1"># Non-contiguous
</span><span class="n">x</span><span class="p">.</span><span class="nf">normal_</span><span class="p">()</span>  <span class="c1"># Should fill with random values
</span><span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>  <span class="c1"># Prints 0.0 - the operation silently failed!
</span></code></pre></div></div> <p>Yet again, the operations complete without error, but the tensor remains unchanged‚Äîthe kernel computes random values into a temporary contiguous buffer but never copies them back.</p> <p>Having just traced through this exact bug pattern, I recognized it immediately and knew exactly how to fix it. Filed an <a href="https://github.com/pytorch/pytorch/issues/165257" rel="external nofollow noopener" target="_blank">Issue</a> and made a <a href="https://github.com/pytorch/pytorch/pull/165267" rel="external nofollow noopener" target="_blank">PR</a> applying the same solution.</p> <p>I suspect there are other similar bugs lying around, as none of these fixes actually address the underlying quirk that <strong>the Placeholder abstraction itself is problematic when used with output tensors</strong>.</p> <p>The core issue: Placeholder‚Äôs constructor silently creates a temporary contiguous copy for non-contiguous tensors, but it has no way to know if it‚Äôs wrapping an input (where the copy is fine- we just read from it) or an output (where the copy is broken- results get written to it then lost). This means <strong>every single operation that uses Placeholder for outputs must manually implement the same workaround pattern</strong> or else it has this silent failure:</p> <div class="language-objc highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Every MPS operation must remember to do this:</span>
<span class="n">bool</span> <span class="n">needsCopy</span> <span class="o">=</span> <span class="o">!</span><span class="n">output</span><span class="p">.</span><span class="n">is_contiguous</span><span class="p">();</span>
<span class="n">Tensor</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">needsCopy</span> <span class="p">?</span> <span class="n">at</span><span class="p">:</span><span class="o">:</span><span class="n">empty</span><span class="p">(...)</span> <span class="o">:</span> <span class="n">output</span><span class="p">;</span>
<span class="k">@autoreleasepool</span> <span class="p">{</span>
    <span class="n">Placeholder</span> <span class="n">p</span><span class="p">(</span><span class="n">temp</span><span class="p">);</span>
    <span class="n">runGraph</span><span class="p">();</span>
<span class="p">}</span>
<span class="k">if</span> <span class="p">(</span><span class="n">needsCopy</span><span class="p">)</span>
  <span class="n">output</span><span class="p">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">temp</span><span class="p">);</span>
</code></pre></div></div> <p>This is a leaky abstraction<d-footnote>A "leaky abstraction" is when an abstraction that's supposed to hide implementation details forces you to understand and work around those details anyway. Placeholder is supposed to abstract Metal buffer management, but its internal copying leaks through, forcing every caller to manually handle non-contiguous outputs. See Joel Spolsky's <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/" rel="external nofollow noopener" target="_blank">The Law of Leaky Abstractions</a> for the canonical explanation.</d-footnote>: the internal implementation detail that ‚ÄúPlaceholder makes temporary copies‚Äù has leaked out to every caller, making it each operation‚Äôs responsibility to work around. A better design would be:</p> <ul> <li>Placeholder knows input vs output: Pass a flag so Placeholder can handle the copy-back itself</li> <li>Separate abstractions: Different wrapper types for inputs (InputPlaceholder) and outputs (OutputPlaceholder)</li> <li>Make the temporary explicit: Don‚Äôt hide the copy inside Placeholder‚Äîmake callers explicitly create and manage contiguous temporaries (this is what I used in the fixes for addcmul_/addcdiv_/the random ops)</li> </ul> <p>The good news: macOS 15+ Metal now handles non-contiguous tensors natively, making this entire issue obsolete for newer systems. But for anyone on older macOS versions or maintaining PyTorch‚Äôs MPS backend, this abstraction continues to cause issues.</p> <p>So ideally, the Placeholder class would be redesigned to handle output tensors correctly by default, but given that the hardware is moving to handle this natively anyway, the pragmatic fix is probably just to audit and patch the remaining operations using the established pattern.</p> <h3 id="practical-takeaways-for-your-code">Practical Takeaways for Your Code</h3> <p><strong>Performance Considerations</strong></p> <p>Even with the code fixes, non-contiguous tensors on MPS involve: Allocate temporary buffer -&gt; Copy to contiguous layout -&gt; Compute -&gt; Copy back. Making tensors contiguous once at initialization avoids thousands of copies during training! And even if your OS can avoid making this temporary contiguous copy, it is still slower to operate on non-contiguous memory if you will be using it many times.</p> <p><strong>When to Call <code class="language-plaintext highlighter-rouge">.contiguous()</code></strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># When to call .contiguous() - General Principles
</span>
<span class="c1"># 1. After operations that change memory layout:
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Non-contiguous
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>          <span class="c1"># Might fail if non-contiguous!
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Safe
</span>
<span class="c1"># 2. Before operations that might not handle strides:
# - Custom CUDA/Metal kernels  
# - Newer backend features
# - Operations that failed mysteriously on certain devices
</span>
<span class="c1"># 3. For performance on repeated operations:
</span><span class="n">weights</span> <span class="o">=</span> <span class="nf">init_weights</span><span class="p">().</span><span class="n">T</span>   <span class="c1"># Used in every forward pass
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">()</span>  <span class="c1"># Pay copy cost once, not every iteration
</span>
<span class="c1"># But don't overuse it!
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>  <span class="c1"># Creates new contiguous tensor anyway
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">()</span>  <span class="c1"># Unnecessary copy!
</span></code></pre></div></div> <p><strong>For MPS specifically:</strong> If on macOS &lt;15, make sure all your parameters are contiguous!</p> <h3 id="what-i-learned">What I Learned</h3> <p><strong>Isolate to specific, measurable symptoms.</strong> The most standard advice and for such good reason. Everything got easier once I had a concrete target: ‚Äú<code class="language-plaintext highlighter-rouge">exp_avg_sq</code> stays at zero‚Äù is infinitely more debuggable than ‚Äúthe loss plateaus mysteriously.‚Äù Once I had a specific symptom, I could strip away components and test the minimal case that triggered it.</p> <p><strong>When debugging tensor issues, check metadata not just values.</strong> I was checking for NaNs, visualizing weights, inspecting gradients‚Äîall focused on the numbers inside tensors. The actual problem was the tensor‚Äôs <em>stride pattern</em>. Device, dtype, contiguity, memory layout‚Äîthese aren‚Äôt just performance details, they can cause silent correctness bugs. <code class="language-plaintext highlighter-rouge">tensor.is_contiguous()</code> is now part of my debugging checklist.</p> <p><strong>When I‚Äôm confused, I might have changed two things‚Äîor there might be two bugs.</strong> Switching to fp64 ‚Äúfixed‚Äù it, but I‚Äôd also switched from MPS to CPU. Untangling that revealed the real culprit. And <code class="language-plaintext highlighter-rouge">exp_avg_sq</code> staying zero <em>should</em> have caused explosions, but the parameter update <em>also</em> failed‚Äîone bug perfectly masked the other.</p> <p><strong>Documentation makes more sense when I need it.</strong> I‚Äôd skimmed PyTorch internals docs before and nothing stuck‚Äîdispatch systems, stride patterns, kernel implementations all felt overwhelming. But once I <em>had</em> to understand how <code class="language-plaintext highlighter-rouge">addcmul_</code> dispatches to MPS kernels, everything clicked. Now PyTorch feels less like a black box. And when I hit the random ops bug weeks later, I wasn‚Äôt intimidated‚ÄîI knew exactly how to trace through the source.</p> <p><strong>Explore the system before exploring the code.</strong> When I needed to debug <code class="language-plaintext highlighter-rouge">addcmul_out_mps</code> in unfamiliar MPS code, I ran experiments first: which operations fail? Do they run at all? What triggers the bug? By the time I opened the source, I knew to compare <code class="language-plaintext highlighter-rouge">addcmul_</code> (broken) against <code class="language-plaintext highlighter-rouge">mul_</code> (working) and scan specifically for differences in output handling. Without that context, I‚Äôd have been lost in Objective-C++ with no idea what mattered. Also LLMs were very helpful with unfamiliar constructs like <code class="language-plaintext highlighter-rouge">MPSGraphTensor</code> or <code class="language-plaintext highlighter-rouge">@autoreleasepool</code>, although they‚Äôre still less reliable with MPS than more documented frameworks.</p> <p><strong>Write post-mortems‚Äì even for yourself.</strong> Forcing myself to explain <em>why</em> I tried each debugging step was as educational as the original investigation. It‚Äôs like experience replay in RL: you explore many failed paths, find one that works, then replay that successful trajectory to reinforce the policy. Writing it down builds pattern recognition‚Äîwhen I‚Äôm in ‚Äúsituation A‚Äù, what hypotheses are worth trying? I‚Äôve written lower-effort debugging debriefs before, but making this one readable for an external audience forced me to articulate why each step made sense, deepening my understanding of what actually worked.</p> <p>What started as a frustrating research roadblock became a surprisingly fun &amp; educational detour. It forced a closer look at things normally taken for granted: Adam‚Äôs momentum mechanics, stride patterns, kernel dispatch. Understanding why each operation behaved differently revealed more about PyTorch‚Äôs architecture than typical usage ever does.</p> <hr> <p>If you made it this far, thanks for joining! Hope you had fun and/or learned something &amp; happy debugging!</p> <p>Special thanks to <a href="https://x.com/nickevanjoseph" rel="external nofollow noopener" target="_blank">Nicholas Joseph</a>, <a href="https://www.benkuhn.net/" rel="external nofollow noopener" target="_blank">Ben Kuhn</a>, and <a href="https://www.alextamkin.com/" rel="external nofollow noopener" target="_blank">Alex Tamkin</a> for giving feedback on this üíú</p> <div class="newsletter-signup" style="margin-top: 3rem; padding-top: 2rem; border-top: 1px solid #ddd;"> <div class="ml-embedded" data-form="8ucYhU"></div> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-910Q4XEB22"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-910Q4XEB22");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>