---
layout: distill
title: The Case of the Frozen Autoencoder
description: A debugging story where a mysterious plateau forces us to understand Adam's internals, tensor memory layouts, device dispatch, and Metal kernel implementations
tags:
giscus_comments: false
date: 2025-10-09
featured: true
thumbnail: 
authors:
  - name: Elana Simon
    url: "https://www.elanapearl.github.io"
    affiliations:
      name: Stanford University
images:
  compare: true
  slider: true
og_image: 
og_image_width: 2126
og_image_height: 1478
twitter_card: summary_large_image
twitter_image: 
toc:
  - name: The Mystery- A Plateauing Loss
  - name: Isolating the Problem
    subsections:
      - name: Are Gradients Flowing?
      - name: Is It the Optimizer?
      - name: How Adam Works
      - name: Examining Adam's State
      - name: Testing Hypotheses
  - name: Understanding Device Differences
    subsections:
      - name: Why the Same Code Behaves Differently
      - name: Why Does Only the Encoder Hit This Bug?
  - name: Tensor Memory Layouts
    subsections:
      - name: Shape vs Memory- The Logical vs Physical View
      - name: Standard Contiguous Matrix Layout
      - name: Transposed (Non-Contiguous) Layout
      - name: How My Encoder Became Non-Contiguous
      - name: Testing the Hypothesis
  - name: Finding the Broken Operations
    subsections:
      - name: What Does Adam Actually Do?
      - name: The Complete Failure Chain
      - name: How One Bug Masked Another
  - name: Comparing Working vs Broken Implementations
    subsections:
      - name: PyTorch's Dispatch System
      - name: The Working Implementation
      - name: The Broken Implementation
      - name: Why Metal Requires Special Handling
  - name: The Fix and Lessons Learned
    subsections:
      - name: The Fix
      - name: The Plot Twist
      - name: Practical Takeaways for Your Code
      - name: What I Learned
---
Training loss plateaued and wouldn't budge. What looked like a standard ML problem (bad hyperparameters? buggy loss function?) was actually a tricky bug hunt that descended through almost every layer of PyTorch. To understand why it broke, I had to understand how each layer actually works - Adam's state management, tensor memory layouts, PyTorch's dispatch system, and finally the Metal kernel implementations.

I had a surprisingly fun time diving into this, learned a lot, and wrote up the whole journey. So if you enjoy debugging mysteries or find that tracking down bugs teaches you more than documentation ever could, this might resonate. We go through the investigation step-by-step, explaining the framework internals as they become necessary to crack the case üïµÔ∏è‚Äç‚ôÄÔ∏è

_Also debugging post-mortems sometimes make me worry I wouldn't have been smart enough / had enough background knowledge to figure them out on my own, so I tried to make this accessible and explain as we go, as I really think any ML eng could have done this if curious / stubborn enough._

<details>
<summary><b>TL;DR - Just tell me the bug</b></summary>
<div>
  <p><strong>The Bug:</strong> A PyTorch GPU kernel bug silently failed when writing to non-contiguous memory, causing my model's encoder weights to freeze during training on Apple Silicon (MPS backend, PyTorch &lt;2.4).</p>

  <p><strong>The Technical Details:</strong> PyTorch's MPS (Apple Silicon GPU) backend had a kernel bug where <code>addcmul_</code> and <code>addcdiv_</code> operations silently fail when writing to non-contiguous output tensors.</p>

  <p><strong>Why It Caused the Training Plateau:</strong></p>
  <ul>
    <li>Encoder weights initialized as transpose of decoder &rarr; non-contiguous memory layout</li>
    <li>Adam's state tensors inherited this layout (<code>exp_avg</code> and <code>exp_avg_sq</code> became non-contiguous)</li>
    <li>MPS kernels for <code>addcmul_</code>/<code>addcdiv_</code> don't handle non-contiguous outputs correctly</li>
    <li>Results computed but written to temporary buffer instead of actual tensor</li>
    <li>For the non-contiguous encoder's Adam parameters, <code>exp_avg_sq.addcmul_()</code> doesn't update &rarr; value stays zero, then the parameter update via <code>addcdiv_</code> also fails &rarr; complete silent freeze</li>
  </ul>

  <p><strong>The Fix:</strong> Make weights contiguous at initialization, or upgrade to PyTorch &ge;2.4</p>
  
  <p><strong>Reproduction:</strong> A minimal reproduction script is available at [GitHub link to be added]</p>
</div>
</details>

---

## The Mystery: A Plateauing Loss

Training loss plateaued way too early. This felt like a standard hyperparameter issue‚Äî but I'd trained this same architecture on similar data with similar hyperparameters countless times and hit much lower losses.

Still, I tried everything: varied learning rates from 1e-5 to 1e-2, tested different schedules, simplified the loss function. Nothing made a difference.

The architecture itself is straightforward‚Äîa two-layer sparse autoencoder (encoder ‚Üí sparse hidden layer ‚Üí decoder), <d-footnote>This is a sparse autoencoder with tied weights - the encoder is initialized as the decoder's transpose. It has some unusual training quirks (TopK sparsity, gradient manipulation, auxiliary losses) but none that should cause this specific issue.</d-footnote>. Small enough that I was training on my M3 MacBook Pro and simple enough I could actually inspect what each layer was doing.

After the standard checks turned up nothing, I started looking at the weights directly‚Äîchecking for NaNs, weird patterns, anything that might explain the plateau. With only two layers, this was pretty feasible.

I visualized the weights at initialization and after the first few training steps. The decoder weights were updating‚Äîvalues shifting, gradients being applied, nothing crazy. But the encoder weights... weren't updating at all. No NaNs, no suspicious patterns‚Äîthey just... weren't changing. They stayed exactly at their initialized values, down to the last decimal place.

Both layers participate in the same forward and backward pass. Why would one update and the other freeze completely?

## Isolating the Problem

### Are Gradients Flowing?

First check: are gradients even making it back to the encoder? Maybe I broke the gradient flow from the decoder to encoder somehow with the TopK operation or the manual gradient modification?

After `loss.backward()`, the gradient statistics were:

|                | **Encoder**        | **Decoder**       |
|----------------|-------------------|-------------------|
| **Max Grad**   | 1.2e7 (huge!)     | 3.4e5 (normal)    |
| **Sparsity**   | 95% zeros (sparse) | 0% zeros (dense)  |

The encoder gradients were there‚Äîand they were huge (1e7)! Yes, they were sparse (95% zeros) because of how the TopK sparsity works, but there were still plenty of non-zero gradients. So gradients are definitely flowing.

### Is It the Optimizer?

Since the gradients exist but weights aren't updating, the optimizer must be doing something wrong. Testing with manual gradient descent:

```python
# Manual SGD update
with torch.no_grad():
    model.encoder.weight -= 0.001 * model.encoder.weight.grad
# Encoder weights change! ‚úì

# But with Adam...
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer.step()
# Encoder weights don't change! ‚úó
```

{% include question_box.liquid emoji="ü§î" content="The issue is localized to Adam specifically! But why would Adam fail on the encoder but work perfectly on the decoder?" %}

To debug this, we need to understand exactly what Adam does and how it differs from simple gradient descent.

### How Adam Works

<details open>
<summary><b>Understanding Adam's Algorithm (click to collapse if familiar)</b></summary>
<h3>Two Problems with Vanilla SGD</h3>

<p>Standard gradient descent (SGD) updates all parameters the same way:</p>

<pre><code class="language-python"># SGD: one learning rate for everything
param = param - learning_rate * gradient
</code></pre>

<p>This creates two fundamental problems:</p>

<ol>
  <li>
    <b>Different parameters need different learning rates.</b>
    Some parameters might consistently get gradients around 1000 while others get 0.01. With SGD's fixed learning rate, you're stuck: either you move too slowly on small gradients or you overshoot wildly on large ones.
  </li>
  <li>
    <b>The learning rate needs to change over time.</b>
    Early in training, you want big steps to explore the space. Later, you need tiny steps to settle into a minimum. SGD requires manually decaying the learning rate on a schedule.
  </li>
</ol>

<h3>Adam's Solution: Per-Parameter Adaptive Learning Rates</h3>

<p>Adam solves both problems by keeping a running history for each parameter. It maintains two pieces of state:</p>

<ol>
  <li>
    <b>First moment (<code>exp_avg</code>)</b>: Running average of gradients ‚Äì "which direction have we been consistently moving?"
  </li>
  <li>
    <b>Second moment (<code>exp_avg_sq</code>)</b>: Running average of squared gradients ‚Äì "how much do gradients vary in this direction?"
  </li>
</ol>

<p>Here's the simplified algorithm:</p>

<pre><code class="language-python"># Initialize state (done once per parameter)
exp_avg = zeros_like(param)      # First moment
exp_avg_sq = zeros_like(param)   # Second moment
step = 0

# Each training step:
exp_avg = Œ≤‚ÇÅ * exp_avg + (1 - Œ≤‚ÇÅ) * grad              # Update momentum
exp_avg_sq = Œ≤‚ÇÇ * exp_avg_sq + (1 - Œ≤‚ÇÇ) * grad¬≤       # Update variance

# Bias correction (compensate for zero initialization)
# Early steps: moments are biased toward zero since they start at 0
# Correction: divide by (1 - Œ≤^step), which starts small and ‚Üí 1
exp_avg_corrected = exp_avg / (1 - Œ≤‚ÇÅ^step)
exp_avg_sq_corrected = exp_avg_sq / (1 - Œ≤‚ÇÇ^step)
step += 1

# Adapt the step size using both corrected moments
param = param - lr * exp_avg_corrected / (sqrt(exp_avg_sq_corrected) + Œµ)
</code></pre>

<p><b>What Each Part Does:</b></p>
<ul>
  <li>
    <b>First moment (<code>exp_avg</code>)</b>: Smooths out noisy gradients by averaging recent directions ‚Äì like momentum in physics. Default Œ≤‚ÇÅ=0.9 means "90% of last step + 10% new gradient"
  </li>
  <li>
    <b>Second moment (<code>exp_avg_sq</code>)</b>: Adapts per-parameter learning rates. Parameters with large/variable gradients get scaled down (divided by large <code>sqrt(exp_avg_sq)</code>), while consistent small gradients get boosted (divided by small <code>sqrt(exp_avg_sq)</code>)
  </li>
  <li>
    <b>Bias correction</b>: Since both moments start at zero, their early values are biased downward. The correction factor <code>(1 - Œ≤^step)</code> compensates for this: at step 1 it's small (large correction), and it approaches 1 as training progresses (no correction needed), effectively making the beta values vary over time
  </li>
  <li>
    <b>Epsilon (Œµ=1e-8)</b>: Prevents division by zero
  </li>
</ul>

<p>
  For a deeper dive into Adam's design and intuition, check out
  <a href="https://cs231n.github.io/neural-networks-3/#update" target="_blank">Stanford's CS231n notes on optimization</a>.
</p>

</details>

### Examining Adam's State

Looking at Adam's state for our frozen encoder:

Checking the optimizer state revealed the issue. The maximum values in each tensor were:

|                | **Encoder** | **Decoder** |
|----------------|-------------|-------------|
| **exp_avg**    | 3.45e+05    | 2.1e+05     |
| **exp_avg_sq** | 0           | 4.45e+12    |

Wait, WHAT?! The encoder's `exp_avg_sq` is zero despite having momentum accumulated in `exp_avg`.

This feels mathematically impossible... The second moment (`exp_avg_sq`) is zero despite non-zero gradients. Since `exp_avg_sq` stores squared gradients, it should NEVER be zero if gradients are non-zero.

And if it truly were zero, we'd see massive weight updates:


```
param_update = lr √ó exp_avg / (‚àöexp_avg_sq + Œµ)
             = 0.001 √ó 3.45e5 / (‚àö0 + 1e-8)
             = 345 / 1e-8
             = 3.45e10  ‚Üê HUGE!
```

This would be **huge**! But we see NO updates at all. Something seems broken inside Adam.

### Testing Hypotheses

#### Could it be bias correction?
Adam uses bias correction to counteract zero initialization. Having previously encountered subtle training issues due to Adam bias initialization bugs, I wondered if the correction might be broken here.

<aside>
üí°If you haven't been hurt by a bias correction bug before, check out <a href="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for">these</a> <a href="https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation/237282#237282">examples</a> to learn the importance of getting this step right!
</aside>

Recall, the bias correction is effectively just making our beta values dependent on the step index, so if the issue has to do with bias correction, it might have some relation to our beta parameters or step index.

I tested with different beta values, at different steps, and even Œ≤‚ÇÇ=0 (which bypasses the exponential average entirely, making `exp_avg_sq = grad¬≤` directly). The encoder's `exp_avg_sq` still stayed zero, making bias correction seem less likely as a culprit.

Plus, `exp_avg` updated correctly despite using the same bias correction mechanism. So maybe something else is preventing `exp_avg_sq` from updating.

#### Is it a precision issue?
My largest gradients were huge (1e7), and squared that's 1e14. While that _is_ massive, it shouldn't overflow in float32. However, I've also been hurt by precision bugs before so, as with bias correction, I had to try it anyway.

I moved everything to float64... **AND IT STARTED WORKING!**

<div style="
  margin: 2rem 0;
  padding: 2rem;
  background: repeating-linear-gradient(
    45deg,
    color-mix(in srgb, var(--global-theme-color) 8%, var(--global-bg-color)),
    color-mix(in srgb, var(--global-theme-color) 8%, var(--global-bg-color)) 10px,
    color-mix(in srgb, var(--global-theme-color) 12%, var(--global-bg-color)) 10px,
    color-mix(in srgb, var(--global-theme-color) 12%, var(--global-bg-color)) 20px
  );
  border: 1px solid color-mix(in srgb, var(--global-theme-color) 20%, transparent);
  border-radius: 8px;
  font-family: 'Comic Sans MS', cursive, sans-serif;
  color: var(--global-text-color);
  line-height: 1.7;
  position: relative;
  overflow: hidden;
">

<div style="
  position: absolute;
  top: 10px;
  right: 15px;
  font-size: 2rem;
  opacity: 0.4;
  transform: rotate(15deg);
">üòµ‚Äçüí´</div>

<span style="font-size: 1.2em; font-weight: bold; color: var(--global-theme-color);">Wait... WHY is this a precision issue?!</span>

<p style="margin: 1rem 0; font-style: italic; color: color-mix(in srgb, var(--global-text-color) 85%, transparent);">
I asked Claude what could be going on & was told there are some intermediate calculations in Adam that use a wider range and I'm probably overflowing those (even though none of my variables should exceed 10^32)...</p>

<p style="color: var(--global-text-color);">...I now felt confused about what steps of Adam I was missing and also was not clear how an overflow would result in a 0 (I assume we'd get an inf/NaN but maybe we divide by the inf somewhere? or maybe there's an error correction step? or maybe we're actually underflowing somewhere... but that shouldn't lead to ALL zeros?!)
</p>

<p style="margin: 1rem 0; font-weight: bold; color: var(--global-theme-color);">
...But going to fp64 <em>DID</em> fix the issue and LLMs are probably more familiar with PyTorch than I am so I must be missing something... But where was this secret intermediate I'd never heard of? Claude wouldn't tell me and I didn't see it anywhere... 
</p>

<div style="text-align: center; margin-top: 1.5rem; font-size: 1.1em; color: var(--global-theme-color);">
<em>so now what?</em> 
</div>
</div>




After a few more minutes of spiraling<d-footnote>
I know you're probably not reading this for the mental rabbit holes but honestly if a debugging adventure didn't have a moment of spiraling and losing your sense of confidence, did it even happen? Also feels disingenuous to not include this part. And hey, maybe one of these theories could've been correct so was worth considering!
</d-footnote>, I realized that when I switched to float64, I _also_ had to switch from MPS (Apple Silicon GPU) to CPU, since MPS doesn't support float64. I'd changed two variables at once, so maybe it wasn't precision!

Testing with float32 on CPU... **the weights update!!**

{% include question_box.liquid emoji="üí°" content="It wasn't precision at all - it's device-specific! The same float32 code works on CPU but fails on MPS." %}

Ôπ° This is progress!!

Ôπ°  Note to self... have more confidence when debugging & stop letting LLMs confuse you

Ôπ° Now I just need to figure out why the bug only occurs with MPS

## Understanding Device Differences

### Why the Same Operation Behaves Differently on Different Chips

PyTorch's device abstraction lets you write the same code and run it on CPUs, GPUs, and even Apple Silicon. It _feels_ like the same computation is running everywhere ‚Äî but under the hood, each device has its own entirely separate implementation.

When you call a tensor operation like `matmul`, PyTorch looks at the tensor's **metadata** (dtype, shape, etc.) and dispatches to a **specialized kernel**: a low-level, highly optimized function written in C++, CUDA, or Metal, tailored for a specific hardware backend.

So when you write something like:

```python
result = tensor_a @ tensor_b
```
You're not invoking a universal multiply function. PyTorch uses the tensors' metadata to select a device- and dtype-specific kernel that performs the actual computation.

Here's a simplified view of what that dispatch looks like:

```
Python Code: result = tensor_a @ tensor_b
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Examine tensor metadata:         ‚îÇ
    ‚îÇ ‚Ä¢ device (CPU/CUDA/MPS)          ‚îÇ
    ‚îÇ ‚Ä¢ dtype (float32/int64/...)      ‚îÇ  
    ‚îÇ ‚Ä¢ layout (strided/sparse)        ‚îÇ
    ‚îÇ ‚Ä¢ requires_grad (true/false)     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
      Route to appropriate kernel
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚Üì             ‚Üì        ‚Üì       ‚Üì
  ‚Ä¢ CPU kernel   ‚Ä¢ CUDA   ‚Ä¢ MPS   ‚Ä¢ Other
    (C++ SIMD)     kernel   kernel  devices...

```
Multiplying two tensors on the CPU uses a completely different kernel than on MPS or CUDA. Even on the same device, changing the dtype or layout can trigger a different kernel. PyTorch maintains a large set of these implementations to support all the combinations.


We'll see exactly how this dispatch system works in C++ later when we dive into the source code. For now, the important point is: **same Python code ‚Üí different tensor metadata ‚Üí different kernel code ‚Üí different efficiency / bugs.**

In my case, because I'm running this on my M3 MacBook Pro, I'm using MPS (Metal Performance Shaders), which is the GPU backend for Apple Silicon. While it feels a bit crazy to assume that my training plateau is due to a kernel bug, it's a bit less surprising on MPS as it's newer and less mature than the CPU and CUDA backends. (And honestly, most people training/debugging ML models are not doing it on their MacBooks.)

### Why Does Only the Encoder Hit This Bug?

The Adam bug appears when working with the encoder on MPS. What makes the encoder different from the decoder that would trigger different behavior? Maybe something about this tensor is hitting an edge case in a MPS kernel or maybe dispatching computations to a different kernel?

I tested everything I could think of that might differentiate the two tensors:

- Different gradient scales
- Dense vs sparse gradient patterns  
- Removing decoder-specific gradient transformations
- Making encoder and decoder gradients statistically identical

Nothing helped. Even when both tensors had similar gradient statistics, only the encoder's `exp_avg_sq` stayed frozen. The difference wasn't in the _values_ of the tensor - something else about the encoder tensor itself was triggering the bug.

#### What's Actually Inside a PyTorch Tensor?

A tensor is more than just an array of numbers ‚Äî it's a structured object that contains both the numerical data and metadata describing how to interpret and operate on that data.

- **The actual data** (stored in a separate `Storage` object)
- **Metadata:**
  - **Device** (CPU, CUDA, MPS) ‚Äì where the data lives
  - **Dtype** (float32, int64, etc.) ‚Äì type of numbers
  - **Shape** ‚Äì logical dimensions
  - **Requires_grad** ‚Äì whether to track gradients
  - **Stride** ‚Äì how to navigate memory

Checking tensor metadata properties:

|                | **Encoder**     | **Decoder**     | **Same?** |
|----------------|-----------------|-----------------|-----------|
| **Device**     | mps:0           | mps:0           | ‚úì         |
| **Dtype**      | float32         | float32         | ‚úì         |
| **Shape**      | [384, 1536]     | [1536, 384]     | ‚úì         |
| **Requires_grad** | True         | True            | ‚úì         |
| **Stride**     | (1, 1536)       | (1536, 1)       | ‚ùå        |
| **Contiguous** | False           | True            | ‚ùå        |

{% include question_box.liquid emoji="üí°" content="The encoder is non-contiguous while the decoder is contiguous!" %}

This gives us a lead to follow üéâüéâ

But we need to understand the implications: **Why would non-contiguous memory layout affect Adam's operations?**

## Tensor Memory Layouts

### Shape vs Memory: The Logical vs Physical View

When you create a tensor, you're dealing with two different concepts:

1. **Shape** (logical dimensions): How you think about the data
   - `shape = (2, 3)` means "2 rows, 3 columns" 
   - This is the conceptual structure you work with

2. **Memory layout** (physical storage): How the data is actually stored
   - Memory is always a flat, 1D array of numbers
   - The tensor's **stride** tells PyTorch how to map between logical positions and memory locations

**What Are Strides?**

**Stride = how many elements to skip when moving to the next position in each dimension.**


Let's start with a simple 1D example before moving to matrices:

**1D Tensor Example:**
```
Full tensor [a, b, c, d, e]:
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ a ‚îÇ b ‚îÇ c ‚îÇ d ‚îÇ e ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
  0   1   2   3   4   (indices)

Shape: (5,)
Stride: (1,)  - move 1 element to get next value

Every other element [::2]:
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ a ‚îÇ b ‚îÇ c ‚îÇ d ‚îÇ e ‚îÇ  ‚Üê actual memory (unchanged)
‚îî‚îÄ‚î¨‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚î¨‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚î¨‚îÄ‚îò
  a       c       e     ‚Üê logical view

Shape: (3,)
Stride: (2,)  - skip one element to get next value
```

Now let's see how this works with 2D matrices:

For a tensor with `shape = (2, 3)` (2 rows, 3 columns):
- **Row stride**: How many elements to skip to move from one row to the next
- **Column stride**: How many elements to skip to move from one column to the next

### Standard Contiguous Matrix Layout
```
Logical view (2x3 matrix):        Memory layout:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1   2   3      ‚îÇ               ‚îÇ 1 ‚îÇ 2 ‚îÇ 3 ‚îÇ 4 ‚îÇ 5 ‚îÇ 6 ‚îÇ
‚îÇ  4   5   6      ‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 0   1   2   3   4   5   (indices)

Shape: (2, 3)
Stride: (3, 1)
- To go from row 0 to row 1: skip 3 elements (index 0‚Üí3)
- To go from col 0 to col 1: skip 1 element (index 0‚Üí1)

Reading order: [0][0]=index 0, [0][1]=index 1, [0][2]=index 2,
               [1][0]=index 3, [1][1]=index 4, [1][2]=index 5
```

### Transposed (Non-Contiguous) Layout
```
After .T (transpose):
Logical view (3x2 matrix):        SAME Memory layout:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1   4      ‚îÇ                   ‚îÇ 1 ‚îÇ 2 ‚îÇ 3 ‚îÇ 4 ‚îÇ 5 ‚îÇ 6 ‚îÇ
‚îÇ  2   5      ‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  3   6      ‚îÇ                     0   1   2   3   4   5
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Shape: (3, 2)  (swapped!)
Stride: (1, 3)  (swapped!)
- To go from row 0 to row 1: skip 1 element (index 0‚Üí1)
- To go from col 0 to col 1: skip 3 elements (index 0‚Üí3)

Reading order: [0][0]=index 0, [0][1]=index 3,
               [1][0]=index 1, [1][1]=index 4,
               [2][0]=index 2, [2][1]=index 5

Notice: We jump around in memory (0‚Üí3‚Üí1‚Üí4‚Üí2‚Üí5) instead of sequential access!
```

PyTorch reuses the same memory with different stride information - no data is moved!

### Common Operations That Create Non-Contiguous Tensors

**There's only one contiguous layout for any given shape, but countless non-contiguous possibilities.** Here are common operations that create non-contiguous tensors:

**Transpose: Swaps Stride Order**
```
Original (2√ó3):          After .T (3√ó2):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1   2   3      ‚îÇ      ‚îÇ  1   4      ‚îÇ
‚îÇ  4   5   6      ‚îÇ  ‚Üí   ‚îÇ  2   5      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ  3   6      ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Stride: (3, 1)           Stride: (1, 3) - non-contiguous!
```

**Strided Slicing: Changes Step Size**
```
data[::2, :] - every other row:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  row 0          ‚îÇ  ‚Üí   ‚îÇ  row 0          ‚îÇ
‚îÇ  row 1 (skip)   ‚îÇ      ‚îÇ  row 2          ‚îÇ
‚îÇ  row 2          ‚îÇ      ‚îÇ  row 4          ‚îÇ
‚îÇ  row 3 (skip)   ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  row 4          ‚îÇ      Stride: (4000, 1) - non-contiguous!
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Dimension Permuting: Reorders Dimensions**
```python
x_3d = torch.randn(2, 3, 4)          # Shape: (2, 3, 4), Stride: (12, 4, 1)
x_perm = x_3d.permute(2, 0, 1)       # Shape: (4, 2, 3), Stride: (1, 12, 4) - non-contiguous!
```

*Advanced indexing (e.g., `tensor[[0, 2, 4], :]`) can also create non-contiguous results, though not always.*

### Why Operations That Create Non-Contiguous Tensors Are Fast

Reading and doing math on top of non-contiguous tensors seems confusing... why would we do this? 

The key insight is that operations like transpose, reshape, and slice are **instant** because they don't actually move any data in memory - they just adjust the tensor's metadata (stride, shape, etc.) to change how PyTorch navigates the same underlying memory. This makes these operations incredibly fast since no data copying is required.

But what's the performance cost of making them contiguous?

```python
# Measure the cost of forcing contiguous layout
x = torch.randn(2000, 2000, device='cpu')

# Transpose: view vs copy
x_t_noncontig = x.T                    # Non-contiguous view (0.076ms)
x_t_contig = x.T.contiguous()          # Contiguous copy (3.055ms)

# Strided slice: view vs copy
data = torch.randn(5000, 2000)
slice_noncontig = data[::2, :]         # Non-contiguous view (0.061ms)
slice_contig = data[::2, :].contiguous()  # Contiguous copy (2.229ms)
```

Creating views is essentially free (microseconds), while forcing contiguous layout requires copying the entire tensor (milliseconds). This is why PyTorch defaults to preserving stride patterns - it avoids unnecessary copies.

### How My Encoder Became Non-Contiguous

Looking at the weight initialization code:
```python
self.encoder.weight.data = self.decoder.weight.T.clone()
```

The `.T` creates a non-contiguous view, and surprisingly, `.clone()` preserves the stride pattern even though it copies to new memory! This is why only the encoder is non-contiguous!

`.clone()` has an optional argument that clarifies what memory layout to use when cloning that you can specify via [torch.memory_format](https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format) and it defaults to keeping the same memory layout as the original (`torch.preserve_format`)!

..But wait, I thought the whole point of keeping tensors non-contiguous is to save time by _avoiding_ cloning... if we're already cloning why would we ever want to preserve a non-contiguous layout?!

Turns out that it is _still_ faster to preserve a memory layout _even when you're already cloning the data!_:
```python
x_t = x.T  # Start with non-contiguous
y_noncontig = x_t.clone()              # Preserves non-contiguous (1.919ms)
y_contig = x_t.clone(memory_format=torch.contiguous_format)  # Force contiguous (4.401ms)
```

### Testing the Hypothesis

If we set `model.encoder.weight.data = model.encoder.weight.contiguous()`, we can force the encoder weight to use a contiguous memory layout and now Adam happily updates and the model trains!!

{% include question_box.liquid title="Remaining Question" emoji="‚ùì" content="Why does a <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>non-contiguous encoder weight</code> cause <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>zero second moment</code> and <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>no parameter updates</code> with Adam on MPS?" %}

## Finding the Broken Operations

### What Does Adam Actually Do?

When Adam updates parameters, what operations does it perform? Let's look at [PyTorch's Adam implementation](https://github.com/pytorch/pytorch/blob/main/torch/optim/adam.py).

Fair warning: this file is over 1000 lines! To find what we need, search for where `exp_avg` and `exp_avg_sq` are defined and updated.

Here are the critical lines ([lines 101, 391-407](https://github.com/pytorch/pytorch/blob/39901f229520a5256505ec24782f716ee7ddc843/torch/optim/adam.py#L101)):

```python
# State initialization (line 101)
state["exp_avg"] = torch.zeros_like(param, memory_format=torch.preserve_format)
state["exp_avg_sq"] = torch.zeros_like(param, memory_format=torch.preserve_format)

# ... [300 lines of setup and parameter group handling] ...

# First moment update (line 391)
exp_avg.lerp_(grad, 1 - beta1)

# Second moment update (line 392)
exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

# ... [bias correction calculations] ...

# Parameter update (line 407)
param.addcdiv_(exp_avg, denom, value=-step_size)
```

Look at that initialization! `memory_format=torch.preserve_format` means the state tensors inherit their stride pattern from `param`. So when our encoder weight is non-contiguous, both `exp_avg` and `exp_avg_sq` are also non-contiguous.

But they're BOTH non-contiguous - so why does only one break?

Well, there are multiple different operations for addition / multiplication used, any one of which could be a suspect, so let's test each one individually! In PyTorch, when a function name ends with an underscore (like `mul_`), that indicates that it is performing an **in-place operation** to modify a tensor directly in memory. Just as different devices can distinct kernels, so can distinctions like these!

For operations like `output.addcmul_(input1, input2)`, the **output tensor** is modified while **input tensors** are read from. In our case, we know the output tensor is non-contiguous, so let's test if that is sufficient to cause our bug:

Testing each Adam operation with non-contiguous output tensors:

| **Operation** | **Function** | **Result** |
|---------------|--------------|------------|
| Linear interpolation | `lerp_()` | Updates ‚úì |
| Scalar multiply | `mul_()` | Updates ‚úì |
| Add + multiply | `addcmul_()` | Stays zero ‚úó |
| Add + divide | `addcdiv_()` | Stays zero ‚úó |

{% include question_box.liquid emoji="üí°" content="Found it! <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>addcmul_()</code> and <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>addcdiv_()</code> both fail silently with non-contiguous outputs on MPS." %}

Interestingly, _input contiguity doesn't matter_ - only the output! Whether `grad`, `exp_avg`, or `denom` are contiguous makes no difference for calculating `exp_avg_sq`. The bug is purely in how these kernels write to non-contiguous _output_ buffers.

### Testing the Broken Operations

To understand what's happening, I tested each operation with different tensor layouts and devices. The critical insight: **the operations aren't producing zeros or NaNs - they're simply not modifying the output tensor at all.**

**MPS Operation Timing:**

| Operation | Contiguous (C) | Non-Contiguous (NC) | Notes |
|-----------|------------|----------------|-----------------|
| `mul_` | 0.7ms | 1.5ms | (+0.8ms) for NC - output updated |
| `addcmul_` | 1.1ms | 2.1ms | (+1.0ms) for NC - output unchanged |
| `addcdiv_` | 1.4ms | 2.3ms | (+0.9ms) for NC - output unchanged|

Non-contiguous operations take significantly longer (~2x), proving MPS is treating them differently and doing substantial work. Yet for `addcmul_` and `addcdiv_`, this work produces no output changes whatsoever - if I start with a tensor of 5s, it remains 5s after the operation.

This timing pattern reduces the chance of this being some type of simple bug, but why are these MPS-specific kernels for in-place operations not modifying the output tensor?

While we don't yet know _what_ is wrong with these kernels, we can trace the entire sequence that triggers our specific failure:

### The Complete Failure Chain

**Step 1: Initialization**
```python
# Creates non-contiguous encoder weight (stride: 1, 1536)
encoder.weight = decoder.weight.T.clone()
```

**Step 2: Adam State Creation**
```python
# Both state tensors inherit non-contiguous layout from param
state["exp_avg"] = zeros_like(param, memory_format=torch.preserve_format)
state["exp_avg_sq"] = zeros_like(param, memory_format=torch.preserve_format)
```

**Step 3: Optimization Loop**

*First moment update:*
```python
exp_avg.lerp_(grad, 1-Œ≤‚ÇÅ)  # ‚úì Works fine
```

*Second moment update:*
```python
exp_avg_sq.mul_(Œ≤‚ÇÇ)                        # ‚úì Works fine
exp_avg_sq.addcmul_(grad, grad, 1-Œ≤‚ÇÇ)      # ‚úó No update - stays zero!
```

**Step 4: Parameter Update**
```python
# Should update param, does nothing, leading to silent failure
param.addcdiv_(exp_avg, denom, value=-step_size)  # ‚úó No update!
```

**Result:** Everyone's favorite bug! A complete parameter freeze with no error messages üôÉ

### How One Bug Masked Another

Note that the second bug actually masked the first bug, creating a perfectly silent failure:

**If ONLY `exp_avg_sq.addcmul_()` failed (Bug 1):**
- `exp_avg_sq` stays zero ‚Üí update = lr * exp_avg / sqrt(Œµ) ‚Üí HUGE updates
- Weights would explode - we'd notice immediately!

**But with `param.addcdiv_()` also failing (Bug 2):**
- `exp_avg_sq` stays zero (bug 1) 
- This _should_ cause huge parameter updates
- But the parameter update operation _also_ fails (bug 2)
- Result: Complete silent freeze - looks like no learning is happening
- No "smoking gun" pointing to where the problem is

The second bug perfectly masked the first! If the parameter update had worked correctly, the zero `exp_avg_sq` would have caused catastrophic weight explosions that we'd notice instantly. Instead, we got a subtle plateau that looked like a hyperparameter issue.

While we have made so much progress on this case, we're still not done yet!!

{% include question_box.liquid title="Remaining Question" emoji="üîç" content="Why do <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>addcmul_</code> and <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>addcdiv_</code> fail to update non-contiguous outputs while <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>mul_</code> and <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>lerp_</code> work fine?" %}

## Finding and Fixing the Kernel Bug

To understand why some operations work and others don't, I needed to look at PyTorch's source code for the buggy kernels.

While I normally like to trace through a Python codebase with a debugger, that doesn't work with `tensor.addcmul_()`. When you call this function, there's no Python source code executing - instead, Python immediately jumps into compiled C++ code for performance. And since PyTorch ships as a pre-compiled binary, I can't see that C++ code either.

<details>
<summary><strong>How can Python call C++ functions?</strong></summary>
<div markdown="1">

How can a Python tensor object have methods that execute C++ code? I skipped over this earlier but even though I know PyTorch isn't the only framework to do this and everything is just machine code if you zoom in close enough... it still feels a bit magical to casually call another language. The explanation is **Python bindings**.

When you install PyTorch, you're not just getting Python files. You're also getting compiled C++ libraries (.so files on Linux/Mac, .dll on Windows) that contain the actual mathematical operations. The Python part is essentially a wrapper that:

1. Takes your Python arguments (`tensor`, `other_tensor`, etc.)
2. Converts them to C++ data structures 
3. Calls the appropriate C++ function
4. Converts the C++ result back to a Python tensor
5. Returns it to your Python code

PyTorch uses [pybind11](https://pybind11.readthedocs.io/) to automatically generate this wrapper code. For example, the C++ function signature:

```cpp
Tensor& addcmul_(Tensor& self, const Tensor& tensor1, const Tensor& tensor2, const Scalar& value)
```

Gets automatically wrapped so you can call it from Python as:

```python
tensor.addcmul_(tensor1, tensor2, value=1.0)
```

This is why PyTorch operations are fast despite being called from Python - the heavy lifting happens in optimized C++ code, with Python just handling the interface.

</div>
</details>

And as we discussed earlier, PyTorch dispatches based on tensor metadata, so there isn't just _one_ implementation - there are device-specific kernels for CPU, CUDA, MPS, etc. Since my PyTorch installation just has the compiled binary files, to investigate the actual implementations, we need to clone PyTorch's repository.

### PyTorch's Dispatch System

All kernels are listed in an **operation registry** - a YAML file that maps operation names (like `addcmul_`) to their tensor-specific C++ implementations. In practice, when PyTorch is compiled (normally done before you install it), this registry is used to automatically generate hundreds of scripts that do the actual dispatching based on the patterns described here, but if we just want to understand what kernel our tensor is calling, we can look through the registry.

Searching for "addcmul_" in the registry `native_functions.yaml`:

```yaml
- func: addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
  # our addcmul_ function just points us to the yaml for addcmul.out
  structured_delegate: addcmul.out

# The function addcmul_ points to:
- func: addcmul.out(...)
  dispatch:
    CPU, CUDA: addcmul_out
    MPS: addcmul_out_mps  # Different function for MPS!
```

### The Broken Implementation

`addcmul_out_mps` is defined in [`aten/src/ATen/native/mps/operations/PointwiseOps.mm`](https://github.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/mps/operations/PointwiseOps.mm)<d-footnote><b>Fun fact:</b> MPS kernels are written in Objective-C++ (hence the weird <code>.mm</code> extension), so you'll see the occasional <code>@</code> syntax mixed in with regular C++. It's the same language Apple uses for iOS apps, though the MPS code is mostly C++ with just some Apple framework calls sprinkled in. </d-footnote>:

The first thing I notice is that `addcmul_out_mps` and `addcdiv_out_mps` are both calling the same function (`addc_mul_div_out_mps`) with different args, which explains why both are broken.

Here are some snippets of code that highlight how the output tensor is handled:

```cpp
// Trimmed version of the code
static void addc_mul_div_out_mps(...) {
  // Early returns - no output validation
  if (value.toDouble() == 0.0) {
    output.copy_(self);
    return;
  }
  if (output.numel() == 0) {
    return;
  }

  // ... graph creation code ...
  
  // Output is directly used as placeholder - no contiguity check
  Placeholder outputPlaceholder = Placeholder(cachedGraph->outputTensor, output);
  
  // Results written directly to output tensor
  NSDictionary* results = @{
    outputPlaceholder.getMPSGraphTensor() : outputPlaceholder.getMPSGraphTensorData()
  };
  
  // Runs the graph to execute the operations
  runMPSGraph(mpsStream, cachedGraph->graph(), feeds, results);
  // No post-processing or contiguity handling
}
```
The output gets wrapped in a Placeholder (which converts PyTorch tensors to Metal's buffer format), then added to the results dictionary so MPS knows where to write the computation results.

I don't see any particular handling of memory layouts in this code. If our encoder was accidentally triggering the early returns, that would explain the lack of output, but seems unlikely since non-contiguous tensors take longer to run than contiguous tensors. There are a number of pieces in the code I'm unfamiliar with that could contain the bug (Placeholders, the MPSGraph that executes their models, how the outputs get added to the results dictionary, etc.) so instead of deep-diving all of them, let's just compare this kernel to one that we _know_ works with non-contiguous outputs, `mul_`.

### Comparint to a Working Implementation

The registry leads us to the implementation for `mul_` by `binaryOpTensor` in [BinaryOps.mm](https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/mps/operations/BinaryOps.mm).<d-footnote>Technically it gets dispatched to mul_mps_out which then just calls the BinaryOps helper function.</d-footnote>

Comparing the high level flow of the two functions side by side:
```
addcmul_out_mps (broken)          binaryOpTensor (working, used by mul_)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê           ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

function starts                    function starts
‚îÇ                                  ‚îÇ
‚îÇ                                  ‚îú‚îÄ check if output contiguous
‚îÇ                                  ‚îú‚îÄ if not: create contiguous temp
‚îÇ                                  ‚îÇ
‚îú‚îÄ @autoreleasepool {              ‚îú‚îÄ @autoreleasepool {
‚îÇ   ‚îú‚îÄ Placeholder(output)         ‚îÇ   ‚îú‚îÄ Placeholder(temp or output)
‚îÇ   ‚îú‚îÄ build graph                 ‚îÇ   ‚îú‚îÄ build graph
‚îÇ   ‚îú‚îÄ runMPSGraph()               ‚îÇ   ‚îú‚îÄ runMPSGraph()
‚îÇ   }                              ‚îÇ   }
‚îÇ                                  ‚îÇ
‚îÇ                                  ‚îú‚îÄ if used temp: copy to output
‚îÇ                                  ‚îÇ
‚îî‚îÄ function ends                   ‚îî‚îÄ function ends
```

Okay this helps point out the differences:

* The working version checks contiguity before creating Placeholder
* If non-contiguous, it creates a contiguous temporary
* After computation, it copies results back to the original output
* The broken version does none of this

But why is this pattern necessary? What does Placeholder do that requires this extra handling?

### Why Metal Requires Special Handling

Looking at the Placeholder constructor in [OperationUtils.mm](https://github.com/pytorch/pytorch/blob/6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0/aten/src/ATen/native/mps/OperationUtils.mm#L315C1-L358C2):

```cpp
Placeholder::Placeholder(..., const Tensor& src, ..., bool gatherTensorData, ...)
    : _tensor(src) {
  id<MTLBuffer> srcBuf = getMTLBufferStorage(src);
  
  // If non-contiguous, create contiguous copy
  if ((!src.is_contiguous() || ...) && gatherTensorData) {
    _tensor = gatherViewTensor(src, emptyShell);
    if (!_tensor.has_storage()) {
      _tensor = src.clone(MemoryFormat::Contiguous);
    }
    srcBuf = getMTLBufferStorage(_tensor);  // Point to copy's buffer
  }
  
  // Wrap buffer for Metal
  _value = [[[MPSGraphTensorData alloc] initWithMTLBuffer:srcBuf ...] autorelease];
}
```

**What Placeholder does:** Converts PyTorch tensors to Metal-compatible buffers. For non-contiguous tensors, it creates a contiguous copy stored in its `_tensor` member variable.

**The critical issue:** This copy only lives as long as the Placeholder object. When Placeholder is destroyed, `_tensor` and its buffer are freed.

### The Bug: Non-Contiguous Outputs Lose Their Results

```
Non-contiguous output @ 0x1000 ‚Üê where results should go
    ‚Üì
Placeholder._tensor = contiguous copy @ 0x2000
    ‚Üì
Metal writes results to 0x2000 ‚úì
    ‚Üì
Placeholder destroyed ‚Üí buffer @ 0x2000 freed ‚Üí results lost!
    ‚Üì
Original output @ 0x1000 never modified ‚úó
```

**When does Placeholder's behavior cause issues?**

| Tensor Type | Contiguous? | Issue? | Why? |
|-------------|-------------|---------|------|
| Input | No | ‚úì No | Metal reads from copy, original doesn't need updating |
| Input | Yes | ‚úì No | No copy made, Metal reads from original |
| Output | No | ‚úó **Yes** | Metal writes to copy, results lost when Placeholder destroyed |
| Output | Yes | ‚úì No | No copy made, Metal writes directly to original |

---

## The Fix and Lessons Learned

### The Fix

After understanding the bug, the solution is straightforward - apply the same pattern that `binaryOpTensor` uses:

**The broken implementation:**
```cpp
static void addc_mul_div_out_mps(..., Tensor& output, ...) {
  @autoreleasepool {
    Placeholder outputPlaceholder = Placeholder(..., output);
    runMPSGraph(...);
  }
  // No copy-back!
}
```

**The fixed implementation:**
```cpp
Tensor output = output_;
bool needsCopyToOutput = false;

if (!output_.is_contiguous()) {
    output = at::empty(...);  // Create contiguous buffer WE manage
    needsCopyToOutput = true;
}

@autoreleasepool {
    Placeholder outputPlaceholder = Placeholder(..., output);
    runMPSGraph(...);
}

if (needsCopyToOutput) {
    output_.copy_(output);  // Copy results back
}
```

**Why this works:**
1. **Manages buffer lifetime outside Placeholder** - `output` survives Placeholder's destruction
2. **Explicitly copies results back** - from contiguous `output` to non-contiguous `output_`
3. **Bonus optimization: Uses `at::empty()` instead of clone** - allocates buffer without copying data, saving ~1ms since output values will be completely overwritten anyway (even though `output_` participates in the computation as `self`, we don't need its old values in the output buffer)

I tested this locally and it worked! The encoder weights finally updated and the model trained successfully.

### A Lesson in Version Control

After implementing this fix locally, I started to make a PR with the fix & some tests since this is a spooky bug to be hanging around, that might go unnoticed as it mostly affects GPU-poor people like me trying to train ML models on their laptops...

I realized I should check if newer PyTorch versions had addressed this.. and turns out an ML eng at Apple had fixed this exact bug in an very similar way to what I did <d-footnote> The official fix uses `at::empty_like(self, MemoryFormat::Contiguous)` instead of `at::empty()` to create the temporary buffer, and checks contiguity using a `needsGather()` helper instead of `!is_contiguous()`. The variable naming is also reversed (output_ is the temp, output is original), and it uses a different runMPSGraph API signature. However, the core pattern is identical: detect non-contiguous output, create a contiguous temporary buffer, perform the computation, then copy results back to the original tensor.</d-footnote>. Apparently Apple even added an optimization for macOS 15+ where Metal now handles non-contiguous tensors natively!

While I now felt pretty silly for diving so deep on an already-fixed bug, the process was still pretty fun & educational & worth the effort... but I probably should've tried upgrading PyTorch earlier in the process.

### Practical Takeaways for Your Code

**Performance Considerations**

Even with the fix, non-contiguous tensors on MPS involve:
1. Allocate temporary buffer
2. Copy to contiguous layout
3. Compute
4. Copy back

Making tensors contiguous once at initialization avoids thousands of copies during training:
```python
# Better performance on MPS
self.encoder.weight.data = self.decoder.weight.T.clone().contiguous()
```

**When to Call `.contiguous()`**

```python
# When to call .contiguous() - General Principles

# 1. After operations that change memory layout:
x = tensor.transpose(0, 1)  # Non-contiguous
x = tensor.view(-1)          # Might fail if non-contiguous!
x = x.contiguous().view(-1)  # Safe

# 2. Before operations that might not handle strides:
# - Custom CUDA/Metal kernels  
# - Newer backend features
# - Operations that failed mysteriously on certain devices

# 3. For performance on repeated operations:
weights = init_weights().T   # Used in every forward pass
weights = weights.contiguous()  # Pay copy cost once, not every iteration

# But don't overuse it!
x = x + y  # Creates new contiguous tensor anyway
x = x.contiguous()  # Unnecessary copy!
```

### What I Learned

**Silent failures are brutal to debug.** No errors, no NaNs, just weights that don't move. If you want to disentangle the effects of your hyperparameters / architecture from actual code bugs -- change up the device, dtype, and code versions ASAP and see if anything changes.

**Tensor metadata matters for correctness, not just speed.** I knew non-contiguous tensors could be slower, but I didn't expect them to break operations entirely. Strides and memory layout are first-class debugging targets now, not just performance considerations.

**Simplify and isolate ASAP.** This one I already knew but it felt so important here. There were _so_ many potential culprits (some I skipped here for brevity) - things got so much easier once I found a specific symptom to test against (`exp_avg_sq`=0 is so much easier to debug than _idk the loss is plateauing?_) and then removed components from my training to see which were necessary to trigger the bug.

**Scope your questions for LLMs carefully.** When I tried asking an LLM to just debug the whole thing for me I got _nowhere_ and kept getting responses like 'it just seems like an inconsistent bug! maybe numerical instabilities' which generally made me question myself. But on the flip side, when I asked for help explaining the kernel code or questions about PyTorch internals, LLMs were _super_ helpful - they couldn't figure out the bug but once I understood it, they were very helpful at implementing / testing the fix (the fix was easy but knowing how to test it locally was not!). This really reminded me to constantly calibrate to what types of tasks they are/aren't currently helpful with. When I tried asking details about what was breaking in the MPS code I was sent down incorrect rabbit holes, and when LLM's responses are too vague, instead of assuming I'm just confused, I should remind myself _it_ may be confused (even though it appears more confident than I am).

**Keep dependencies up to date.** In my repo, I had another package that required an earlier PyTorch version, so I hadn't prioritized updating PyTorch. It's also just scary to be updating everything mid-project incase results change-- but since then I've prioritized taking the time to make improvements that make it lower effort to quickly update (i.e. minimize dependencies with fixed versions, have lots of tests so you know if upgrading changes any data/results, etc.)

**Check if it's already fixed.** I felt silly discovering a bug that was patched months ago, but honestly? The debugging process taught me more about PyTorch internals than reading docs ever would. Sometimes the journey matters more than the destination.

**Post-mortem your bugs.** Forcing myself to re-tell the story of how I explored this and why I tried each thing I did has been honestly just as educational as the first process of doing it in real time. It really forces you to go over the logical steps of 'when you are in situation A, here are the hypotheses worth considering and how to test them' and even if you don't encounter them exactly again it's SO helpful in case you do or even just for other debugging. Think of it like RL: you try a bunch of debugging experiments, some are bad ideas, some are good ideas. Forcing yourself to explain the good ideas is like reinforcing the rollouts that succeeded. Even if you just write it for yourself it's helpful, although this is the first time I've written a post-mortem for anyone but myself to read, and I *do* think it led to more learning.

-----

Anyway- if you made it this far, thanks for joining! Hope you had fun and/or learned something & happy debugging üíú