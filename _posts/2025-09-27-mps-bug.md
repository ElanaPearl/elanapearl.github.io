---
layout: distill
title: The Case of the Frozen Autoencoder
description: A debugging story where a mysterious plateau forces us to understand Adam's internals, tensor memory layouts, device dispatch, and Metal kernel implementations
tags: []
giscus_comments: false
date: 2025-10-09
featured: true
thumbnail: ""
authors:
  - name: Elana Simon
    url: "https://www.elanapearl.github.io"
    affiliations:
      name: Stanford University
images:
  compare: true
  slider: true
og_image: ""
og_image_width: 2126
og_image_height: 1478
twitter_card: summary_large_image
twitter_image: "" 
syntax_theme_light: jekyll-pygments-themes-one-light.css
syntax_theme_dark: jekyll-pygments-themes-one-dark-pro.css
toc:
  - name: The Mystery- A Plateauing Loss
  - name: Isolating the Problem
    subsections:
      - name: Are Gradients Flowing?
      - name: Is It the Optimizer?
      - name: How Adam Works
      - name: Examining Adam's State
      - name: Testing Hypotheses
  - name: Device-Specific Differences
    subsections:
      - name: Why the Same Code Behaves Differently
      - name: Why Does Only the Encoder Hit This Bug?
  - name: Tensor Memory Layouts
    subsections:
      - name: Shape vs Memory- The Logical vs Physical View
      - name: Standard Contiguous Matrix Layout
      - name: Transposed (Non-Contiguous) Layout
      - name: How My Encoder Became Non-Contiguous
      - name: Testing the Hypothesis
  - name: Identifying the Broken Operations
    subsections:
      - name: What Does Adam Actually Do?
      - name: Putting the Pieces Together 
      - name: How One Bug Masked Another
  - name: Inside the Kernel Implementation
    subsections:
      - name: PyTorch's Dispatch System
      - name: The Broken Implementation
      - name: Comparing to a Working Implementation
      - name: Why Metal Requires Special Handling
  - name: Case Closed 
    subsections:
      - name: The Fix
      - name: A Lesson in Version Control
      - name: The Pattern Strikes Again
      - name: Practical Takeaways for Your Code
      - name: What I Learned
---
Training loss plateaued and wouldn't budge. What looked like a standard ML problem (bad hyperparameters? buggy loss function?) was actually a tricky bug hunt that descended through almost every layer of PyTorch. To understand why it broke, I had to understand how each layer actually works - Adam's state management, tensor memory layouts, PyTorch's dispatch system, and finally the Metal kernel implementations.

I had a surprisingly fun time diving into this, learned a lot, and wrote up the whole journey. So if you enjoy debugging mysteries or find that tracking down bugs teaches you more than documentation ever could, this might resonate. We go through the investigation step-by-step, explaining the framework internals as they become necessary to crack the case ğŸ•µï¸â€â™€ï¸

_Also debugging post-mortems sometimes make me worry I wouldn't have been smart enough / had enough background knowledge to figure them out on my own, so I tried to make this accessible and explain as we go, as I really think any ML engineer could have done this if curious / stubborn enough._

<details>
<summary><b>TL;DR - Just tell me the bug</b></summary>
<div markdown="1">

**The Bug:** A PyTorch GPU kernel bug silently failed when writing to non-contiguous memory, causing my model's encoder weights to freeze during training on Apple Silicon (MPS backend, PyTorch <2.4).

**The Technical Details:** PyTorch's MPS (Apple Silicon GPU) backend had a kernel bug where `addcmul_` and `addcdiv_` operations silently fail when writing to non-contiguous output tensors.

**Why It Caused the Training Plateau:**
- Encoder weights initialized as transpose of decoder â†’ non-contiguous memory layout
- Adam's state tensors inherited this layout (`exp_avg` and `exp_avg_sq` became non-contiguous)
- MPS kernels for `addcmul_`/`addcdiv_` don't handle non-contiguous outputs correctly
- Results computed but written to temporary buffer instead of actual tensor
- For the non-contiguous encoder's Adam parameters, `exp_avg_sq.addcmul_()` doesn't update â†’ value stays zero, then the parameter update via `addcdiv_` also fails â†’ complete silent freeze

**The Fix:** Make weights contiguous at initialization, or upgrade to PyTorch â‰¥2.4

**Reproduction:** A minimal reproduction script & walkthrough is available at [GitHub link to be added once i come up with a name for the repo]

</div>
</details>

---

## The Mystery: A Plateauing Loss

Training loss plateaued way too early. This felt like a standard hyperparameter issueâ€” but I'd trained this same architecture on similar data with similar hyperparameters countless times and hit much lower losses.

Still, I tried everything: varied learning rates from 1e-5 to 1e-2, tested different schedules, simplified the loss function. Nothing made a difference.

The architecture itself is straightforwardâ€”a two-layer sparse autoencoder (encoder â†’ sparse hidden layer â†’ decoder)<d-footnote>This sparse autoencoder has some unusual training features (TopK sparsity, auxiliary losses, gradient adjustments, encoder initialized as transpose of decoer) that initially seemed like potential culprits.</d-footnote>. Small enough that I was training on my MacBook Pro and simple enough I could actually inspect what each layer was doing.

After the standard checks turned up nothing, I started looking at the weights directlyâ€”checking for NaNs, weird patterns, anything that might explain the plateau. With only two layers, this was pretty feasible.

I visualized the weights at initialization and after the first few training steps. The decoder weights were updatingâ€”values shifting, gradients being applied, nothing crazy. But the encoder weights... weren't updating at all. No NaNs, no suspicious patternsâ€”they just... weren't changing. They stayed exactly at their initialized values, down to the last decimal place.

Both layers participate in the same forward and backward pass. Why would one update and the other freeze completely?

## Isolating the Problem

### Are Gradients Flowing?

First check: are gradients even making it back to the encoder? Maybe I broke the gradient flow from the decoder to encoder somehow with the TopK operation or the manual gradient modification?

After `loss.backward()`, the gradient statistics were:

|                | **Encoder**        | **Decoder**       |
|----------------|-------------------|-------------------|
| **Max Grad**   | 2.35e6    | 6.64e6      |
| **Sparsity**   | 88.5% zeros  | 88.5% zeros    |

The encoder gradients were thereâ€”and they were pretty big (as intended for my dataset)! Yes, they were sparse (majority zeros) because of how the TopK sparsity works, but there were still plenty of non-zero gradients. So gradients are definitely flowing.

### Is It the Optimizer?

Since the gradients exist but weights aren't updating, the optimizer must be doing something wrong. Testing with manual gradient descent:

```python
# Manual SGD update
with torch.no_grad():
    model.encoder.weight -= 0.001 * model.encoder.weight.grad
# Encoder weights change! âœ“

# But with Adam...
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer.step()
# Encoder weights don't change! âœ—
```

{% include question_box.liquid emoji="ğŸ¤”" content="The issue is localized to Adam specifically! But why would Adam fail on the encoder but work perfectly on the decoder?" %}

Since the gradients exist but weights aren't updating, the optimizer must be doing something wrong. But what exactly does Adam do that could fail?

### How Adam Works

To understand what might be breaking, I need to understand what Adam actually does differently from simple gradient descent.

<details open>
<summary><b>Understanding Adam's Algorithm (click to collapse if familiar)</b></summary>
<div markdown="1">

### Two Problems with Vanilla SGD

Standard gradient descent (SGD) updates all parameters the same way:

```python
# SGD: one learning rate for everything
param = param - learning_rate * gradient
```

This creates two fundamental problems:

1. **Different parameters need different learning rates.**
   Some parameters might consistently get gradients around 1000 while others get 0.01. With SGD's fixed learning rate, you're stuck: either you move too slowly on small gradients or you overshoot wildly on large ones.

2. **The learning rate needs to change over time.**
   Early in training, you want big steps to explore the space. Later, you need tiny steps to settle into a minimum. SGD requires manually decaying the learning rate on a schedule.

### Adam's Solution: Per-Parameter Adaptive Learning Rates

Adam solves both problems by keeping a running history for each parameter. It maintains two pieces of state:

1. **First moment (`exp_avg`)**: Running average of gradients â€“ "which direction have we been consistently moving?"

2. **Second moment (`exp_avg_sq`)**: Running average of squared gradients â€“ "how much do gradients vary in this direction?"

Here's the simplified algorithm:

```python
# Initialize state (done once per parameter)
exp_avg = zeros_like(param)      # First moment
exp_avg_sq = zeros_like(param)   # Second moment
step = 0

# Each training step:
exp_avg = beta_1 * exp_avg + (1 - beta_1) * grad              # Update momentum
exp_avg_sq = beta_2 * exp_avg_sq + (1 - beta_2) * grad**2       # Update variance

# Bias correction (compensate for zero initialization)
# Early steps: moments are biased toward zero since they start at 0
# Correction: divide by (1 - beta^step), which starts small and â†’ 1
exp_avg_corrected = exp_avg / (1 - beta_1^step)
exp_avg_sq_corrected = exp_avg_sq / (1 - beta_2^step)
step += 1

# Adapt the step size using both corrected moments
param = param - lr * exp_avg_corrected / (sqrt(exp_avg_sq_corrected) + Îµ)
```

**What Each Part Does:**

- **First moment (`exp_avg`)**: Smooths out noisy gradients by averaging recent directions â€“ like momentum in physics. Default beta_1=0.9 means "90% of last step + 10% new gradient"

- **Second moment (`exp_avg_sq`)**: Adapts per-parameter learning rates. Parameters with large/variable gradients get scaled down (divided by large `sqrt(exp_avg_sq)`), while consistent small gradients get boosted (divided by small `sqrt(exp_avg_sq)`)

- **Bias correction**: Since both moments start at zero, their early values are biased downward. The correction factor `(1 - Î²^step)` compensates for this: at step 1 it's small (large correction), and it approaches 1 as training progresses (no correction needed), effectively making the beta values vary over time

- **Epsilon (Îµ=1e-8)**: Prevents division by zero

For a deeper dive into Adam's design and intuition, check out [Stanford's CS231n notes on optimization](https://cs231n.github.io/neural-networks-3/#update).

</div>
</details>

Knowing what Adam _should_ be doing, let's look at the state it's maintaining (those `exp_avg` and `exp_avg_sq` tensors that track momentum and variance) to see what it's _actually_ doing.

### Examining Adam's State

For our frozen encoder, the maximum values in each state tensor were:

|                | **Encoder** | **Decoder** |
|----------------|-------------|-------------|
| **exp_avg**    | 1.96e+05    | 1.70e+06    |
| **exp_avg_sq** | <span style="display: inline-block; border: 2px solid var(--global-theme-color); border-radius: 4px; padding: 1px 10px; font-weight: bold;">0</span>           | 1.18e+11    |

Wait, WHAT?! The encoder's `exp_avg_sq` is zero despite having momentum accumulated in `exp_avg`.

This feels mathematically impossible... The second moment (`exp_avg_sq`) is zero despite non-zero gradients. Since `exp_avg_sq` stores squared gradients, it should NEVER be zero if gradients are non-zero.

And if it truly were zero, we'd see massive weight updates.

```
param_update = lr Ã— exp_avg / (âˆšexp_avg_sq + Îµ) # PyTorch calls this denominator "denom"
             = 0.001 Ã— 1.96e5 / (âˆš0 + 1e-8)
             = 196 / 1e-8
             = 1.96e10  â† HUGE!
```

This would be **huge**! Yet we see NO updates... a paradox that points to a deeper issue.

### Testing Hypotheses

#### Could it be bias correction?
Adam uses bias correction to counteract zero initialization. Having previously encountered subtle training issues due to Adam bias initialization bugs, I wondered if the correction might be broken here.

<aside>
ğŸ’¡If you haven't been hurt by a bias correction bug before, check out <a href="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for">these</a> <a href="https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation/237282#237282">examples</a> to learn the importance of getting this step right!
</aside>

Recall, the bias correction is effectively just making our beta values dependent on the step index, so if the issue has to do with bias correction, it might have some relation to our beta parameters or step index.

I tested with different beta values, at different steps, and even beta_2=0 (which bypasses the exponential average entirely, making `exp_avg_sq = grad**2` directly). The encoder's `exp_avg_sq` still stayed zero, making bias correction seem less likely as a culprit.

Plus, `exp_avg` updated correctly despite using the same bias correction mechanism. So maybe something else is preventing `exp_avg_sq` from updating.

#### Is it a precision issue?
My largest gradients were huge (1e7), and squared that's 1e14. While that _is_ massive, it shouldn't overflow in float32. However, I've also been hurt by precision bugs before<d-footnote>Floating point precision issues have a fun habit of causing silent failures/degradations like this one (where it completes but produces incorrect values). Always worth checking, even when it seems unlikely.</d-footnote>, so I had to try it anyway.

I moved everything to float64... **AND IT STARTED WORKING!**

<div style="
  margin: 2rem 0;
  padding: 2rem;
  background: repeating-linear-gradient(
    45deg,
    color-mix(in srgb, var(--global-theme-color) 8%, var(--global-bg-color)),
    color-mix(in srgb, var(--global-theme-color) 8%, var(--global-bg-color)) 10px,
    color-mix(in srgb, var(--global-theme-color) 12%, var(--global-bg-color)) 10px,
    color-mix(in srgb, var(--global-theme-color) 12%, var(--global-bg-color)) 20px
  );
  border: 1px solid color-mix(in srgb, var(--global-theme-color) 20%, transparent);
  border-radius: 8px;
  font-family: 'Comic Sans MS', cursive, sans-serif;
  color: var(--global-text-color);
  line-height: 1.7;
  position: relative;
  overflow: hidden;
">

<div style="
  position: absolute;
  top: 10px;
  right: 15px;
  font-size: 2rem;
  opacity: 0.4;
  transform: rotate(15deg);
">ğŸ˜µâ€ğŸ’«</div>

<span style="font-size: 1.2em; font-weight: bold; color: var(--global-theme-color);">Wait... WHY is this a precision issue?!</span>

<p style="margin: 1rem 0; font-style: italic; color: color-mix(in srgb, var(--global-text-color) 85%, transparent);">
I asked Claude to help me understand the situation & was told there are intermediate calculations in Adam that might overflow...</p>

<p style="color: var(--global-text-color);">...but I couldn't find these mysterious intermediates in the code. And how would an overflow produce exact zeros instead of inf/NaN? Maybe we divide by the inf somewhere? Or there's an error correction step? Or we're underflowing? But that shouldn't give ALL zeros?!
</p>

<p style="margin: 1rem 0; font-weight: bold; color: var(--global-theme-color);">
...Going to fp64 <em>DID</em> fix it though, and LLMs probably know PyTorch better than I do, so maybe I'm missing something obvious? But where was this secret intermediate? I couldn't find it anywhere... 
</p>

<div style="text-align: center; margin-top: 1.5rem; font-size: 1.1em; color: var(--global-theme-color);">
<em>so now what?</em> 
</div>
</div>

After a few more minutes of spiraling<d-footnote>
You're probably not reading this for the mid-debugging-self-doubt, but every debugging adventure has a spiraling moment (at least for me) so feels disingenuous to skip this step. And maybe one of these theories could've actually been correct! </d-footnote>, I realized something: when I switched to float64, I _also_ had to switch from MPS (Apple Silicon GPU) to CPU, since MPS doesn't support float64. **I'd changed two variables at once.**

Testing with float32 on CPU... **the weights update!!**

{% include question_box.liquid emoji="ğŸ’¡" content="Turns out, precision wasn't the culprit, it was <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>device-specific</code>! The exact same float32 code updates weights on CPU but fails on MPS. This was progress: same code, same datatypes, but different devices meant different implementationsâ€”and different bugs." %}

ï¹¡ This is progress!!

ï¹¡ Note to self... simpler explanations are more likely correct- even (and especially!) when LLMs confidently assert complicated theories that are hard to understand / verify

ï¹¡ Now I just need to figure out why the bug only occurs with MPS

## Device-Specific Differences

### Why the Same Operation Behaves Differently on Different Chips

PyTorch's device abstraction lets you write the same code and run it on CPUs, GPUs, and even Apple Silicon. It _feels_ like the same computation is running everywhere â€” but under the hood, each device has its own entirely separate implementation.

When you call a tensor operation like `matmul`, PyTorch looks at the tensor's **metadata** (dtype, shape, etc.) and dispatches to a **specialized kernel**: a low-level, highly optimized function written in C++, CUDA, or Metal, tailored for a specific hardware backend.

So when you write something like:

```python
result = tensor_a @ tensor_b
```
You're not invoking a universal multiply function. PyTorch uses the tensors' metadata to select a device- and dtype-specific kernel that performs the actual computation.

Here's a simplified view of what that dispatch looks like:

```
Python Code: result = tensor_a @ tensor_b
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Examine tensor metadata:         â”‚
    â”‚ â€¢ device (CPU/CUDA/MPS)          â”‚
    â”‚ â€¢ dtype (float32/int64/...)      â”‚  
    â”‚ â€¢ layout (strided/sparse)        â”‚
    â”‚ â€¢ requires_grad (true/false)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
      Route to appropriate kernel
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
     â†“             â†“        â†“       â†“
  â€¢ CPU kernel   â€¢ CUDA   â€¢ MPS   â€¢ Other
    (C++ SIMD)     kernel   kernel  devices...

```
Multiplying two tensors on the CPU uses a completely different kernel than on MPS or CUDA. Even on the same device, changing the dtype or layout can trigger a different kernel. PyTorch maintains a large set of these implementations to support all the combinations.


We'll see exactly how this dispatch system works in C++ later when we dive into the source code. For now, the important point is: **same Python code â†’ different tensor metadata â†’ different kernel code â†’ different efficiency / bugs.**

In my case, because I'm running this on my M3 MacBook Pro, I'm using MPS (Metal Performance Shaders), which is the GPU backend for Apple Silicon. While it feels a bit crazy to assume that my training plateau is due to a kernel bug, it's a bit less surprising on MPS as it's newer and less mature than the CPU and CUDA backends. (And honestly, most people training/debugging ML models are not doing it on their MacBooks.)

### Why Does Only the Encoder Hit This Bug?

The Adam bug appears when working with the encoder on MPS. What makes the encoder different from the decoder that would trigger different behavior? Maybe something about this tensor is hitting an edge case in a MPS kernel or maybe dispatching computations to a different kernel?

I tested everything I could think of that might differentiate the two tensors:

- Different gradient scales
- Dense vs sparse gradient patterns  
- Removing decoder-specific gradient transformations
- Making encoder and decoder gradients statistically identical

Nothing helped. Even when both tensors had similar gradient statistics, only the encoder's `exp_avg_sq` stayed frozen. The difference wasn't in the _values_ of the tensor - something else about the encoder tensor itself was triggering the bug.

#### What's Actually Inside a PyTorch Tensor?

A tensor is more than just an array of numbers â€” it's a structured object that contains both the numerical data and metadata describing how to interpret and operate on that data.

- **The actual data** (stored in a separate `Storage` object)
- **Metadata:**
  - **Device** (CPU, CUDA, MPS) â€“ where the data lives
  - **Dtype** (float32, int64, etc.) â€“ type of numbers
  - **Shape** â€“ logical dimensions
  - **Requires_grad** â€“ whether to track gradients
  - **Stride** â€“ how to navigate memory

Checking tensor metadata properties:

|                | **Encoder**     | **Decoder**     | **Same?** |
|----------------|-----------------|-----------------|-----------|
| **Device**     | mps:0           | mps:0           | âœ“         |
| **Dtype**      | float32         | float32         | âœ“         |
| **Shape**      | [384, 1536]     | [1536, 384]     | âœ“         |
| **Requires_grad** | True         | True            | âœ“         |
| **Stride**     | (1, 1536)       | (1536, 1)       | âŒ        |
| **Contiguous** | False           | True            | âŒ        |

{% include question_box.liquid emoji="ğŸ’¡" content="The encoder is non-contiguous while the decoder is contiguous!" %}

This gives us a lead to follow ğŸ‰ğŸ‰

But we need to understand the implications: **Why would non-contiguous memory layout affect Adam's operations?**

## Tensor Memory Layouts

### Shape vs Memory: The Logical vs Physical View

When you create a tensor, you're dealing with two different concepts:

1. **Shape** (logical dimensions): How you think about the data
   - `shape = (2, 3)` means "2 rows, 3 columns" 
   - This is the conceptual structure you work with

2. **Memory layout** (physical storage): How the data is actually stored
   - Memory is always a flat, 1D array of numbers
   - The tensor's **stride** tells PyTorch how to map between logical positions and memory locations

**What Are Strides?**

**Stride = how many elements to skip when moving to the next position in each dimension.**


Let's start with a simple 1D example before moving to matrices:

**1D Tensor Example:**
```
Full tensor [a, b, c, d, e]:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ a â”‚ b â”‚ c â”‚ d â”‚ e â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
  0   1   2   3   4   (indices)

Shape: (5,)
Stride: (1,)  - move 1 element to get next value

Every other element [::2]:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ a â”‚ b â”‚ c â”‚ d â”‚ e â”‚  â† actual memory (unchanged)
â””â”€â”¬â”€â”´â”€â”€â”€â”´â”€â”¬â”€â”´â”€â”€â”€â”´â”€â”¬â”€â”˜
  a       c       e     â† logical view

Shape: (3,)
Stride: (2,)  - skip one element to get next value
```

Now let's see how this works with 2D matrices:

For a tensor with `shape = (2, 3)` (2 rows, 3 columns):
- **Row stride**: How many elements to skip to move from one row to the next
- **Column stride**: How many elements to skip to move from one column to the next

### Standard Contiguous Matrix Layout
```
Logical view (2x3 matrix):        Memory layout:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚  1   2   3      â”‚               â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚
â”‚  4   5   6      â”‚               â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 0   1   2   3   4   5   (indices)

Shape: (2, 3)
Stride: (3, 1)
- To go from row 0 to row 1: skip 3 elements (index 0â†’3)
- To go from col 0 to col 1: skip 1 element (index 0â†’1)

Reading order: [0][0]=index 0, [0][1]=index 1, [0][2]=index 2,
               [1][0]=index 3, [1][1]=index 4, [1][2]=index 5
```

### Transposed (Non-Contiguous) Layout
```
After .T (transpose):
Logical view (3x2 matrix):        SAME Memory layout:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚  1   4      â”‚                   â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚
â”‚  2   5      â”‚                   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
â”‚  3   6      â”‚                     0   1   2   3   4   5
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Shape: (3, 2)  (swapped!)
Stride: (1, 3)  (swapped!)
- To go from row 0 to row 1: skip 1 element (index 0â†’1)
- To go from col 0 to col 1: skip 3 elements (index 0â†’3)

Reading order: [0][0]=index 0, [0][1]=index 3,
               [1][0]=index 1, [1][1]=index 4,
               [2][0]=index 2, [2][1]=index 5

Notice: We jump around in memory (0â†’3â†’1â†’4â†’2â†’5) instead of sequential access!
```

PyTorch reuses the same memory with different stride information - no data is moved!

### Common Operations That Create Non-Contiguous Tensors

**There's only one contiguous layout for any given shape, but countless non-contiguous possibilities.** Here are common operations that create non-contiguous tensors:

**Transpose: Swaps Stride Order**
```
Original (2Ã—3):          After .T (3Ã—2):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1   2   3      â”‚      â”‚  1   4      â”‚
â”‚  4   5   6      â”‚  â†’   â”‚  2   5      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  3   6      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Stride: (3, 1)           Stride: (1, 3) - non-contiguous!
```

**Strided Slicing: Changes Step Size**
```
data[::2, :] - every other row:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  row 0          â”‚  â†’   â”‚  row 0          â”‚
â”‚  row 1 (skip)   â”‚      â”‚  row 2          â”‚
â”‚  row 2          â”‚      â”‚  row 4          â”‚
â”‚  row 3 (skip)   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  row 4          â”‚      Stride: (4000, 1) - non-contiguous!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dimension Permuting: Reorders Dimensions**
```python
x_3d = torch.randn(2, 3, 4)          # Shape: (2, 3, 4), Stride: (12, 4, 1)
x_perm = x_3d.permute(2, 0, 1)       # Shape: (4, 2, 3), Stride: (1, 12, 4) - non-contiguous!
```

*Advanced indexing (e.g., `tensor[[0, 2, 4], :]`) can also create non-contiguous results, though not always.*

### Why Operations That Create Non-Contiguous Tensors Are Fast

Reading and doing math on top of non-contiguous tensors seems confusing... why would we do this? 

The key insight is that operations like transpose, reshape, and slice are **instant** because they don't actually move any data in memory - they just adjust the tensor's metadata (stride, shape, etc.) to change how PyTorch navigates the same underlying memory. This makes these operations incredibly fast since no data copying is required.

But what's the performance cost of making them contiguous?

```python
# Measure the cost of forcing contiguous layout
x = torch.randn(2000, 2000, device='cpu')

# Transpose: view vs copy
x_t_noncontig = x.T                    # Non-contiguous view (0.076ms)
x_t_contig = x.T.contiguous()          # Contiguous copy (3.055ms)

# Strided slice: view vs copy
data = torch.randn(5000, 2000)
slice_noncontig = data[::2, :]         # Non-contiguous view (0.061ms)
slice_contig = data[::2, :].contiguous()  # Contiguous copy (2.229ms)
```

Creating views is essentially free (microseconds), while forcing contiguous layout requires copying the entire tensor (milliseconds). This is why PyTorch defaults to preserving stride patterns - it avoids unnecessary copies.

### How My Encoder Became Non-Contiguous

Looking at the weight initialization code:
```python
self.encoder.weight.data = self.decoder.weight.T.clone()
```

The `.T` creates a non-contiguous view, and surprisingly, `.clone()` preserves the stride pattern even though it copies to new memory! This is why only the encoder is non-contiguous!

`.clone()` has an optional argument that clarifies what memory layout to use when cloning that you can specify via [torch.memory_format](https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format) and it defaults to keeping the same memory layout as the original (`torch.preserve_format`)!

..But wait, I thought the whole point of keeping tensors non-contiguous is to save time by _avoiding_ cloning... if we're already cloning why would we ever want to preserve a non-contiguous layout?!

Turns out that it is _still_ faster to preserve a memory layout _even when you're already cloning the data!_:
```python
x_t = x.T  # Start with non-contiguous
y_noncontig = x_t.clone()              # Preserves non-contiguous (1.919ms)
y_contig = x_t.clone(memory_format=torch.contiguous_format)  # Force contiguous (4.401ms)
```

### Testing the Hypothesis

If we set `model.encoder.weight.data = model.encoder.weight.contiguous()`, we can force the encoder weight to use a contiguous memory layout and now Adam happily updates and the model trains!!

{% include question_box.liquid title="Remaining Question" emoji="â“" content="Why does a <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>non-contiguous encoder weight</code> cause <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>zero second moment</code> and <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>no parameter updates</code> with Adam on MPS?" %}

But while making weights contiguous fixed the symptoms, I still didn't understand WHY non-contiguity broke Adam on MPS...

## Identifying the Broken Operations

### What Does Adam Actually Do?

When Adam updates parameters, what operations does it perform? Let's look at [PyTorch's Adam implementation](https://github.com/pytorch/pytorch/blob/main/torch/optim/adam.py).

Fair warning: this file is over 1000 lines! To find what we need, search for where `exp_avg` and `exp_avg_sq` are defined and updated.

Here are the critical lines ([lines 101, 391-407](https://github.com/pytorch/pytorch/blob/39901f229520a5256505ec24782f716ee7ddc843/torch/optim/adam.py#L101)):

```python
# State initialization (line 101)
state["exp_avg"] = torch.zeros_like(param, memory_format=torch.preserve_format)
state["exp_avg_sq"] = torch.zeros_like(param, memory_format=torch.preserve_format)

# ... [300 lines of setup and parameter group handling] ...

# First moment update (line 391)
exp_avg.lerp_(grad, 1 - beta1)

# Second moment update (line 392)
exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

# ... [bias correction calculations] ...

# Parameter update (line 407)
param.addcdiv_(exp_avg, denom, value=-step_size)
```

Look at that initialization! `memory_format=torch.preserve_format` means the state tensors inherit their stride pattern from `param`. So when our encoder weight is non-contiguous, both `exp_avg` and `exp_avg_sq` are also non-contiguous.

But they're BOTH non-contiguous - so why does only one break?

Well, while they both are computed via addition and multiplication, they don't use the exact same operations to perform this. Any of these operations could be a suspect, so let's test each one individually!

<aside>In PyTorch, when a function name ends with an underscore (like `mul_`), that indicates that it is performing an **in-place operation** to modify a tensor directly in memory. Just as different devices can distinct kernels, so can distinctions like these!
</aside>

For operations like `output.addcmul_(input1, input2)`, the **output tensor** is modified while **input tensors** are read from. In our case, we know the output tensor is non-contiguous, so let's test if that is sufficient to cause our bug:

Testing each Adam operation with non-contiguous output tensors:

| **Operation** | **Function** | **Result** |
|---------------|--------------|------------|
| Linear interpolation | `lerp_()` | Updates âœ“ |
| Scalar multiply | `mul_()` | Updates âœ“ |
| Add + multiply | `addcmul_()` | Stays zero âœ— |
| Add + divide | `addcdiv_()` | Stays zero âœ— |

{% include question_box.liquid emoji="ğŸ”" content="Found it! <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>addcmul_()</code> and <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>addcdiv_()</code> both fail silently when WRITING to non-contiguous outputs on MPS." %}

Interestingly, _input contiguity doesn't matter_ - only the output! Whether `grad`, `exp_avg`, or `denom` are contiguous makes no difference for calculating `exp_avg_sq`. The bug is purely in how these kernels write to non-contiguous _output_ buffers.

### Testing the Broken Operations

To understand what's happening, I tested each operation with different tensor layouts and devices. The critical insight: **the operations aren't producing zeros or NaNs - they're simply not modifying the output tensor at all.**

**MPS Operation Timing:**

| Operation | Contiguous (C) | Non-Contiguous (NC) | Notes |
|-----------|------------|----------------|-----------------|
| `lerp_` | 0.17ms | 0.52ms | (+0.36ms) for NC - output updated |
| `mul_` | 0.05ms | 0.39ms | (+0.34ms) for NC - output updated |
| `addcmul_` | 0.17ms | 0.52ms | (+0.35ms) for NC - output unchanged |
| `addcdiv_` | 0.18ms | 0.54ms | (+0.36ms) for NC - output unchanged|

Non-contiguous operations take significantly longer (~2x), proving MPS is treating them differently and doing substantial work. Yet for `addcmul_` and `addcdiv_`, this work produces no output changes whatsoever - if I start with a tensor of 5s, it remains 5s after the operation.

This timing pattern reduces the chance of this being some type of simple bug, but why are these MPS-specific kernels for in-place operations not modifying the output tensor?

While we don't yet know _what_ is wrong with these kernels, we can trace the entire sequence that triggers our specific failure:

### Putting the Pieces Together

**Step 1: Initialization**
```python
# Creates non-contiguous encoder weight (stride: 1, 1536)
encoder.weight = decoder.weight.T.clone()
```

**Step 2: Adam State Creation**
```python
# Both state tensors inherit non-contiguous layout from param
state["exp_avg"] = zeros_like(param, memory_format=torch.preserve_format)
state["exp_avg_sq"] = zeros_like(param, memory_format=torch.preserve_format)
```

**Step 3: Optimization Loop**

*First moment update:*
```python
exp_avg.lerp_(grad, 1-beta_1)  # âœ“ Works fine
```

*Second moment update:*
```python
exp_avg_sq.mul_(beta_2)                        # âœ“ Works fine
exp_avg_sq.addcmul_(grad, grad, 1-beta_2)      # âœ— No update - stays zero!
```

**Step 4: Parameter Update**
```python
# Should update param, does nothing, leading to silent failure
param.addcdiv_(exp_avg, denom, value=-step_size)  # âœ— No update!
```

**Result:** Everyone's favorite bug! A complete parameter freeze with no error messages ğŸ™ƒ

**The second bug masked the first, creating a perfectly silent failure:**

**If ONLY `exp_avg_sq.addcmul_()` failed:**
- `exp_avg_sq` stays zero â†’ update = lr Ã— exp_avg / âˆš(Îµ) â†’ HUGE updates
- Weights would explode â†’ immediate detection

**But with `param.addcdiv_()` also failing:**
- `exp_avg_sq` stays zero (bug 1)
- This *should* cause huge updates  
- But the parameter update *also* fails (bug 2)
- Result: Complete silent freeze

The second bug perfectly masked the first! If parameter updates had worked correctly, the zero `exp_avg_sq` would have caused catastrophic explosions that we'd notice instantly. Instead, we got a subtle plateau that looked like a hyperparameter issue.

<details>
<summary><b>Side note: Why did forward and backward passes work fine with non-contiguous weights?</b></summary>
<div markdown="1">
If non-contiguous tensors can cause operations to silently fail on MPS, why didn't the forward pass or backward pass break?

The forward and backward passes for `F.linear` use `matmul` for their matrix multiplications, which handle non-contiguous tensors correctly on MPS. Testing confirms that both `matmul` (the `@` operator) and `F.linear` work correctly with non-contiguous input tensors and non-contiguous weight matrices on MPS, including during the backward pass where gradients flow through non-contiguous weights without issues.

The bug is specific to the fused in-place operations that Adam uses for state updates: `addcmul_` and `addcdiv_`. These operations fail silently when writing to non-contiguous output tensors, while other in-place operations like `lerp_` and `mul_` work correctly.
</div>
</details>

While we have made so much progress on this case, we're still not done yet!!

{% include question_box.liquid title="Remaining Question" emoji="ğŸ”" content="Why do <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>addcmul_</code> and <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>addcdiv_</code> fail to update non-contiguous outputs while <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>mul_</code> and <code style='background: var(--global-code-bg-color); color: var(--global-theme-color); padding: 0.2rem 0.4rem; border-radius: 4px; font-size: 0.9em;'>lerp_</code> work fine?" %}

## Inside the Kernel Implementation

To understand why some operations work and others don't, I needed to look at PyTorch's source code for the buggy kernels.

While I normally like to trace through a Python codebase with a debugger, that doesn't work with `tensor.addcmul_()`. When you call this function, there's no Python source code executing - instead, Python immediately jumps into compiled C++ code for performance. And since PyTorch ships as a pre-compiled binary, I can't see that C++ code either.

<details>
<summary><strong>How can Python call C++ functions?</strong></summary>
<div markdown="1">

How can a Python tensor object have methods that execute C++ code? I skipped over this earlier but even though I know PyTorch isn't the only framework to do this and everything is just machine code if you zoom in close enough... it still feels a bit magical to casually call another language. The explanation is **Python bindings**.

When you install PyTorch, you're not just getting Python files. You're also getting compiled C++ libraries (.so files on Linux/Mac, .dll on Windows) that contain the actual mathematical operations. The Python part is essentially a wrapper that:

1. Takes your Python arguments (`tensor`, `other_tensor`, etc.)
2. Converts them to C++ data structures 
3. Calls the appropriate C++ function
4. Converts the C++ result back to a Python tensor
5. Returns it to your Python code

PyTorch uses [pybind11](https://pybind11.readthedocs.io/) to automatically generate this wrapper code. For example, the C++ function signature:

```cpp
Tensor& addcmul_(Tensor& self, const Tensor& tensor1, const Tensor& tensor2, const Scalar& value)
```

Gets automatically wrapped so you can call it from Python as:

```python
tensor.addcmul_(tensor1, tensor2, value=1.0)
```

This is why PyTorch operations are fast despite being called from Python - the heavy lifting happens in optimized C++ code, with Python just handling the interface.

</div>
</details>

And as we discussed earlier, PyTorch dispatches based on tensor metadata, so there isn't just _one_ implementation - there are device-specific kernels for CPU, CUDA, MPS, etc. Since my PyTorch installation just has the compiled binary files, to investigate the actual implementations, we need to clone PyTorch's repository.

### PyTorch's Dispatch System

All kernels are listed in an **operation registry** - a YAML file that maps operation names (like `addcmul_`) to their tensor-specific C++ implementations. In practice, when PyTorch is compiled (normally done before you install it), this registry is used to automatically generate hundreds of scripts that do the actual dispatching based on the patterns described here, but if we just want to understand what kernel our tensor is calling, we can look through the registry.

Searching for "addcmul_" in the registry `native_functions.yaml`:

```yaml
- func: addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
  # our addcmul_ function just points us to the yaml for addcmul.out
  structured_delegate: addcmul.out

# The function addcmul_ points to:
- func: addcmul.out(...)
  dispatch:
    CPU, CUDA: addcmul_out
    MPS: addcmul_out_mps  # Different function for MPS!
```

### The Broken Implementation

`addcmul_out_mps` is defined in [`aten/src/ATen/native/mps/operations/PointwiseOps.mm`](https://github.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/mps/operations/PointwiseOps.mm) with one MPS-specific kernel regardless of memory layout.<d-footnote><b>Fun fact:</b> MPS kernels are written in Objective-C++ (hence the weird <code>.mm</code> extension), so you'll see the occasional <code>@</code> syntax mixed in with regular C++. It's the same language Apple uses for iOS apps, though the MPS code is mostly C++ with just some Apple framework calls sprinkled in. </d-footnote>:

The first thing I notice is that `addcmul_out_mps` and `addcdiv_out_mps` are both calling the same function (`addc_mul_div_out_mps`) with different args, which explains why both are broken.

Here are some snippets of code that highlight how the output tensor is handled:

```objc
// Trimmed version of the code
static void addc_mul_div_out_mps(...) {
  // Early returns - no output validation
  if (value.toDouble() == 0.0) {
    output.copy_(self);
    return;
  }
  if (output.numel() == 0) {
    return;
  }

  // ... graph creation code ...
  
  // Output is directly used as placeholder - no contiguity check
  Placeholder outputPlaceholder = Placeholder(cachedGraph->outputTensor, output);
  
  // Results written directly to output tensor
  NSDictionary* results = @{
    outputPlaceholder.getMPSGraphTensor() : outputPlaceholder.getMPSGraphTensorData()
  };
  
  // Runs the graph to execute the operations
  runMPSGraph(mpsStream, cachedGraph->graph(), feeds, results);
  // No post-processing or contiguity handling
}
```
The output gets wrapped in a Placeholder (which converts PyTorch tensors to Metal's buffer format), then added to the results dictionary so MPS knows where to write the computation results.

I don't see any particular handling of memory layouts in this code. If our encoder was accidentally triggering the early returns, that would explain the lack of output, but seems unlikely since non-contiguous tensors take longer to run than contiguous tensors. There are a number of pieces in the code I'm unfamiliar with that could contain the bug (Placeholders, the MPSGraph that executes their models, how the outputs get added to the results dictionary, etc.) so instead of deep-diving all of them, let's just compare this kernel to one that we _know_ works with non-contiguous outputs, `mul_`.

### Comparint to a Working Implementation

The registry leads us to the implementation for `mul_` by `binaryOpTensor` in [BinaryOps.mm](https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/mps/operations/BinaryOps.mm).<d-footnote>Technically it gets dispatched to mul_mps_out which then just calls the BinaryOps helper function.</d-footnote>

Comparing the high level flow of the two functions side by side:
```
addcmul_out_mps (broken)          binaryOpTensor (working, used by mul_)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function starts                    function starts
â”‚                                  â”‚
â”‚                                  â”œâ”€ âœ“ check if output contiguous
â”‚                                  â”œâ”€ âœ“ if not: create temp
â”‚                                  â”‚
â”œâ”€ @autoreleasepool {              â”œâ”€ @autoreleasepool {
â”‚   â”œâ”€ Placeholder(output)         â”‚   â”œâ”€ Placeholder(temp OR output)
â”‚   â”œâ”€ runMPSGraph()               â”‚   â”œâ”€ runMPSGraph()
â”‚   }  â† temp destroyed here!      â”‚   }
â”‚                                  â”‚
â”‚                                  â”œâ”€ âœ“ if used temp: copy back
â”‚   âœ— results lost!                â”‚  âœ“ results preserved!
â”‚                                  â”‚
â””â”€ function ends                   â””â”€ function ends
```

Okay this helps point out the differences:

* The working version checks contiguity before creating Placeholder
* If non-contiguous, it creates a contiguous temporary
* After computation, it copies results back to the original output
* The broken version does none of this

But why is this pattern necessary? What does Placeholder do that requires this extra handling?

### Why Metal Requires Special Handling

Looking at the Placeholder constructor in [OperationUtils.mm](https://github.com/pytorch/pytorch/blob/6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0/aten/src/ATen/native/mps/OperationUtils.mm#L315C1-L358C2):

```objc
Placeholder::Placeholder(..., const Tensor& src, ..., bool gatherTensorData, ...)
    : _tensor(src) {
  id<MTLBuffer> srcBuf = getMTLBufferStorage(src);
  
  // If non-contiguous, create contiguous copy
  if ((!src.is_contiguous() || ...) && gatherTensorData) {
    _tensor = gatherViewTensor(src, emptyShell);
    if (!_tensor.has_storage()) {
      _tensor = src.clone(MemoryFormat::Contiguous);
    }
    srcBuf = getMTLBufferStorage(_tensor);  // Point to copy's buffer
  }
  
  // Wrap buffer for Metal
  _value = [[[MPSGraphTensorData alloc] initWithMTLBuffer:srcBuf ...] autorelease];
}
```

**What Placeholder does:** Converts PyTorch tensors to Metal-compatible buffers. For non-contiguous tensors, it creates a contiguous copy stored in its `_tensor` member variable.

**The critical issue:** This copy only lives as long as the Placeholder object. When Placeholder is destroyed, `_tensor` and its buffer are freed.

{% include question_box.liquid emoji="ğŸ”" content="Placeholder creates temporary buffers for non-contiguous tensors, but these buffers are destroyed before results are copied back. The computation succeeds but results vanish." %}


### The Bug: Non-Contiguous Outputs Lose Their Results

```
Non-contiguous output @ 0x1000 â† where results should go
    â†“
Placeholder._tensor = contiguous copy @ 0x2000
    â†“
Metal writes results to 0x2000 âœ“
    â†“
Placeholder destroyed â†’ buffer @ 0x2000 freed â†’ results lost!
    â†“
Original output @ 0x1000 never modified âœ—
```

**When does Placeholder's behavior cause issues?**

| Tensor Type | Contiguous? | Issue? | Why? |
|-------------|-------------|---------|------|
| Input | No | âœ“ No | Metal reads from copy, original doesn't need updating |
| Input | Yes | âœ“ No | No copy made, Metal reads from original |
| Output | No | âœ— **Yes** | Metal writes to copy, results lost when Placeholder destroyed |
| Output | Yes | âœ“ No | No copy made, Metal writes directly to original |

---

## Case Closed

### The Fix

After understanding the bug, the solution is straightforward - apply the same pattern that `binaryOpTensor` uses:

**The broken implementation:**
```objc
static void addc_mul_div_out_mps(..., Tensor& output, ...) {
  @autoreleasepool {
    Placeholder outputPlaceholder = Placeholder(..., output);
    runMPSGraph(...);
  }
  // No copy-back!
}
```

**The fixed implementation:**
```objc
Tensor output = output_;
bool needsCopyToOutput = false;

if (!output_.is_contiguous()) {
    output = at::empty(...);  // Create contiguous buffer WE manage
    needsCopyToOutput = true;
}

@autoreleasepool {
    Placeholder outputPlaceholder = Placeholder(..., output);
    runMPSGraph(...);
}

if (needsCopyToOutput) {
    output_.copy_(output);  // Copy results back
}
```

**Why this works:**
1. **Manages buffer lifetime outside Placeholder** - `output` survives Placeholder's destruction
2. **Explicitly copies results back** - from contiguous `output` to non-contiguous `output_`
3. **Bonus optimization: Uses `at::empty()` instead of clone** - allocates buffer without copying data, saving ~1ms since output values will be completely overwritten anyway (even though `output_` participates in the computation as `self`, we don't need its old values in the output buffer)

I tested this locally and it worked! The encoder weights finally updated and the model trained successfully.

### A Lesson in Version Control

After building PyTorch locally to test my fix (which worked!), I realized I was working on PyTorch v2.2.1<d-footnote>Which also inconveniently made it difficult to build and test as I had to downgrade things like CMake to even build this older PyTorch</d-footnote>. 

Checking the latest version revealed the bug was already fixed in v2.4, patched by an ML engineer at Apple using almost the exact same approach I'd discovered<d-footnote>The official fix uses `at::empty_like(self, MemoryFormat::Contiguous)` instead of `at::empty()` to create the temporary buffer, and checks contiguity using a `needsGather()` helper instead of `!is_contiguous()`. The variable naming is also a bit different, and it uses a different runMPSGraph API signature. However, the core pattern is identical: detect non-contiguous output, create a contiguous temporary buffer, perform the computation, then copy results back to the original tensor.</d-footnote>. Apple even added an optimization for macOS 15+ where Metal now handles non-contiguous tensors natively!

{% include question_box.liquid emoji="ğŸ¤¦â€â™€ï¸" content="While I now felt silly for diving so deep on an already-fixed bug, the process was still very fun / educational / so worth the effort<br>But in hindsight, I should've tried upgrading PyTorch earlier." %}

### The Pattern Strikes Again

When writing this up, I stress tested my kernel implementation just for practice and stumbled upon **the same failure pattern** in a bunch of other operations (in the most up-to-date PyTorch this time!)

**All random in-place operations** (`normal_`, `uniform_`, `exponential_`, `random_`, `bernoulli_`) **silently fail when called on non-contiguous tensors on MPS**.

```python
x = torch.zeros(10, 10).T  # Non-contiguous
x.normal_()  # Should fill with random values
print(x.max())  # Prints 0.0 - the operation silently failed!
```
Yet again, the operations complete without error, but the tensor remains unchangedâ€”the kernel computes random values into a temporary contiguous buffer but never copies them back.

Having just traced through this exact bug pattern, I recognized it immediately and knew exactly how to fix it. Filed an [Issue](https://github.com/pytorch/pytorch/issues/165257) and made a [PR](https://github.com/pytorch/pytorch/pull/165267) applying the same solution.

I suspect there are other similar bugs lying around as none of these fixes address the underlying issue that **the Placeholder abstraction itself is problematic when used with output tensors**. 

The core issue: Placeholder's constructor silently creates a temporary contiguous copy for non-contiguous tensors, but it has no way to know if it's wrapping an input (where the copy is fineâ€”just read from it) or an output (where the copy is brokenâ€”results get written to it then lost). This means **every single operation that uses Placeholder for outputs must manually implement the same workaround pattern**:
```objc
// Every MPS operation must remember to do this:
bool needsCopy = !output.is_contiguous();
Tensor temp = needsCopy ? at::empty(...) : output;
@autoreleasepool {
    Placeholder p(temp);
    runGraph();
}
if (needsCopy)
  output.copy_(temp);
```
This is a leaky abstraction<d-footnote>A "leaky abstraction" is when an abstraction that's supposed to hide implementation details forces you to understand and work around those details anyway. The classic example: TCP/IP is supposed to abstract away unreliable networks, but you still need to handle timeouts and retries because the abstraction "leaks." Here, Placeholder is supposed to abstract away Metal buffer management, but its internal copying behavior leaks through, forcing every caller to manually handle non-contiguous outputs. Joel Spolsky's <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">The Law of Leaky Abstractions</a> is the canonical explanation of this concept.</d-footnote>â€”the internal implementation detail that "Placeholder makes temporary copies" has leaked out to every caller, making it each operation's responsibility to work around. A better design would be:

* Placeholder knows input vs output: Pass a flag so Placeholder can handle the copy-back itself
* Separate abstractions: Different wrapper types for inputs (InputPlaceholder) and outputs (OutputPlaceholder)
* Make the temporary explicit: Don't hide the copy inside Placeholderâ€”make callers explicitly create and manage contiguous temporaries (this is what I used in the fixes for addcmul_/addcdiv_/the random ops)

The good news: macOS 15+ Metal now handles non-contiguous tensors natively, making this entire issue obsolete for newer systems. But for anyone on older macOS versions or maintaining PyTorch's MPS backend, this abstraction continues to cause issues.

So ideally, the Placeholder class would be redesigned to handle output tensors correctly by default, but given that the hardware is moving to handle this natively anyway, the pragmatic fix is probably just to audit and patch the remaining operations using the established pattern.


### Practical Takeaways for Your Code

**Performance Considerations**

Even with the fix, non-contiguous tensors on MPS involve:
1. Allocate temporary buffer
2. Copy to contiguous layout
3. Compute
4. Copy back

Making tensors contiguous once at initialization avoids thousands of copies during training:
```python
# Better performance on MPS
self.encoder.weight.data = self.decoder.weight.T.clone().contiguous()
```

**When to Call `.contiguous()`**

```python
# When to call .contiguous() - General Principles

# 1. After operations that change memory layout:
x = tensor.transpose(0, 1)  # Non-contiguous
x = tensor.view(-1)          # Might fail if non-contiguous!
x = x.contiguous().view(-1)  # Safe

# 2. Before operations that might not handle strides:
# - Custom CUDA/Metal kernels  
# - Newer backend features
# - Operations that failed mysteriously on certain devices

# 3. For performance on repeated operations:
weights = init_weights().T   # Used in every forward pass
weights = weights.contiguous()  # Pay copy cost once, not every iteration

# But don't overuse it!
x = x + y  # Creates new contiguous tensor anyway
x = x.contiguous()  # Unnecessary copy!
```


### What I Learned

**Isolate to specific, measurable symptoms.** The most standard advice and for such good reason. Everything got easier once I had a concrete target: "`exp_avg_sq` stays at zero" is infinitely more debuggable than "the loss plateaus mysteriously." Once I had a specific symptom, I could strip away components and test the minimal case that triggered it.

**When debugging tensor issues, check metadata not just values.** I was checking for NaNs, visualizing weights, inspecting gradientsâ€”all focused on the numbers inside tensors. The actual problem was the tensor's *stride pattern*. Device, dtype, contiguity, memory layoutâ€”these aren't just performance details, they can cause silent correctness bugs. `tensor.is_contiguous()` is now part of my debugging checklist.

**When I'm confused, I might have changed two thingsâ€”or there might be two bugs.** Switching to fp64 "fixed" it, but I'd also switched from MPS to CPU. Untangling that revealed the real culprit. And `exp_avg_sq` staying zero *should* have caused explosions, but the parameter update *also* failedâ€”one bug perfectly masked the other.

**Documentation makes more sense when I need it.** I'd skimmed PyTorch internals docs before and nothing stuckâ€”dispatch systems, stride patterns, kernel implementations all felt overwhelming. But once I *had* to understand how `addcmul_` dispatches to MPS kernels, everything clicked. Now PyTorch feels less like a black box. And when I hit the random ops bug weeks later, I wasn't intimidatedâ€”I knew exactly how to trace through the source.

**Write post-mortemsâ€”even for yourself.** Forcing myself to explain *why* I tried each debugging step has been as educational as the original investigation. Even private post-mortems build intuition: when I'm in "situation A," what are useful hypotheses? This is my first public one, and making it readable definitely deepened my understanding.

---

If you made it this far, thanks for joining! Hope you had fun and/or learned something & happy debugging ğŸ’œ